<!DOCTYPE html><!--[if lt IE 7]>      <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content="Artificial Neural Network library implemented in Python">
        <meta name="viewport" content="width=device-width">
        <title>Tutorials &mdash; NeuralPy</title>
            <link rel="stylesheet" href="_static/normalize.css" type="text/css">
            <link rel="stylesheet" href="_static/sphinx.css" type="text/css">
            <link rel="stylesheet" href="_static/main.css" type="text/css">
            <link rel="stylesheet" href="_static/flat.css" type="text/css">
            <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
            <link rel="stylesheet" href="_static/font-awesome.min.css" type="text/css">
        <link rel="shortcut icon" href="_static/tinkerer.ico" /><!-- Load modernizr and JQuery -->
        <script src="_static/vendor/modernizr-2.6.2.min.js"></script>
        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="_static/vendor/jquery-1.8.2.min.js"><\/script>')</script>
        <script src="_static/plugins.js"></script>
        <script src="_static/main.js"></script>
        <link rel="alternate" type="application/rss+xml" title="RSS" href="rss.html" /><script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.5',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script><script type="text/javascript" src="_static/underscore.js"></script><script type="text/javascript" src="_static/doctools.js"></script><script type="text/javascript" src="_static/js/google_analytics.js"></script><script type="text/javascript" src="_static/js/script.js"></script><script type="text/javascript" src="_static/js/copybutton.js"></script>

    <script type="text/javascript">
        $(document).ready(function () {
            // Scroll to content if on small screen
            if (screen.width < 480)
            {
                $(document).scrollTop(document.getElementsByTagName("article")[0].offsetTop - 44);
            }
        });
    </script>
<style media="screen" type="text/css">
    #fork_me { display: none; }
    #fork_me img { position: fixed; top: 0; right: 0; border: 0; width: 130px; }
    @media only screen and (min-width: 768px) { #fork_me { display: inline; } }

    .docutils { width: 100%; }
    .docutils td { padding: 10px; }
    .section { word-wrap:break-word; }
    .descname { font-weight: bold; }
    .highlight-python + .figure { margin-top: 20px; }
    .dataframe { text-align: center !important; width: 100%; margin: 10px 0 10px 0; }
    .dataframe td { padding: 5px; }
</style></head>
    <body role="document">
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
        <![endif]-->

      <div id="container"><header role="banner">
            <hgroup>
              <h1><a href="pages/home.html">NeuralPy</a></h1><h2>Neural Networks in Python</h2></hgroup>
          </header>
      <nav role="navigation">
            <ul><li class="main-nav">
                  <a href="pages/home.html">Home</a>
                </li>
              <li class="main-nav">
                  <a href="#">Tutorials</a>
                </li>
              <li class="main-nav">
                  <a href="pages/documentation.html">Documentation</a>
                </li>
              <li class="main-nav">
                  <a href="pages/installation.html">Installation</a>
                </li>
              </ul>
          </nav><div class="main-container" role="main"><div class="main wrapper body clearfix"><article><div class="timestamp postmeta">
            <span>July 04, 2015</span>
        </div>
        <div class="section">
            <h1><a href="2015/07/04/boston_house_prices_dataset.html">Boston house-prices dataset</a></h1>
<p>For this tutorial we are going to predict Boston house prices via Conjugate gradient.</p>
<p>For the beginning we should get some data.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_boston</span><span class="p">()</span>
<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
<p>Let’s look closer into the data.</p>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.00632</td>
      <td>18</td>
      <td>2.31</td>
      <td>0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1</td>
    </tr>
    <tr>
      <td>0.02731</td>
      <td>0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2</td>
    </tr>
    <tr>
      <td>0.02729</td>
      <td>0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2</td>
    </tr>
    <tr>
      <td>0.03237</td>
      <td>0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3</td>
    </tr>
    <tr>
      <td>0.06905</td>
      <td>0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>296</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <td>242</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <td>242</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <td>222</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <td>222</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
  </tbody>
</table><p>Data contains 14 columns.
The last column <span class="docutils literal"><span class="pre">MEDV</span></span> is a median value of owner-occupied homes in $1000’s.
The goal is to predict this prices.
Other columns we can use for Neural Network training.
All columns description you can find below.</p>
<ul class="simple">
<li>CRIM     per capita crime rate by town</li>
<li>ZN       proportion of residential land zoned for lots over 25,000 sq.ft.</li>
<li>INDUS    proportion of non-retail business acres per town</li>
<li>CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</li>
<li>NOX      nitric oxides concentration (parts per 10 million)</li>
<li>RM       average number of rooms per dwelling</li>
<li>AGE      proportion of owner-occupied units built prior to 1940</li>
<li>DIS      weighted distances to five Boston employment centres</li>
<li>RAD      index of accessibility to radial highways</li>
<li>TAX      full-value property-tax rate per $10,000</li>
<li>PTRATIO  pupil-teacher ratio by town</li>
<li>B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</li>
<li>LSTAT    % lower status of the population</li>
</ul>
<p>From data set description we can find that there are 13 continuous attributes (including “class” attribute “MEDV”) and 1 binary-valued attribute.
There is no multiple categorical data, so we don’t need to change feature dimention.
But we already have one problem.
If you look closer, you will find that every column has its own data range.
This situation is a bad thing for Neural Network training, because input values ​​make different contributions to the calculation of the output values.
Bigger values will be more important for Network which can be perceived as invalid assumption based on data.
For example in the first row, in the table above, column <span class="docutils literal"><span class="pre">B</span></span> contains value <cite>396.90</cite> and column <span class="docutils literal"><span class="pre">CRIM</span></span> - <cite>0.00632</cite>.
To fix this issue we should transfrom all columns to get similar ranges.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>

<span class="n">data_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">target_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">MinMaxScaler</span><span class="p">()</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">data_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">target_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
<p>After transformation data looks like this.</p>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>...</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.000000</td>
      <td>0.18</td>
      <td>0.067815</td>
      <td>0</td>
      <td>0.314815</td>
      <td>...</td>
    </tr>
    <tr>
      <td>0.000236</td>
      <td>0.00</td>
      <td>0.242302</td>
      <td>0</td>
      <td>0.172840</td>
      <td>...</td>
    </tr>
    <tr>
      <td>0.000236</td>
      <td>0.00</td>
      <td>0.242302</td>
      <td>0</td>
      <td>0.172840</td>
      <td>...</td>
    </tr>
    <tr>
      <td>0.000293</td>
      <td>0.00</td>
      <td>0.063050</td>
      <td>0</td>
      <td>0.150206</td>
      <td>...</td>
    </tr>
    <tr>
      <td>0.000705</td>
      <td>0.00</td>
      <td>0.063050</td>
      <td>0</td>
      <td>0.150206</td>
      <td>...</td>
    </tr>
  </tbody>
</table><p>All the data is now in the range between 0 and 1.</p>
<p>Then we should split our data set into train and validation.
We use 85% of data for train.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c"># To make result reproducible</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.85</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Now we are ready to build Neural Network which will predict house prices.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">neuralpy</span> <span class="kn">import</span> <span class="n">algorithms</span><span class="p">,</span> <span class="n">layers</span>

<span class="n">cgnet</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">ConjugateGradient</span><span class="p">(</span>
    <span class="n">connection</span><span class="o">=</span><span class="p">[</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">SigmoidLayer</span><span class="p">(</span><span class="mi">13</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">SigmoidLayer</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">OutputLayer</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">],</span>
    <span class="n">method</span><span class="o">=</span><span class="s">'golden'</span><span class="p">,</span>
    <span class="n">show_epoch</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
    <span class="n">optimizations</span><span class="o">=</span><span class="p">[</span><span class="n">algorithms</span><span class="o">.</span><span class="n">LinearSearch</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/cgnet-init.png"><img alt="Conjgate Gradient train" src="_images/cgnet-init.png" style="width: 80%;"/></a>
</div>
<p>We define network with one hidden layer.
Input size for this layer is 50.
This value is just a guess.
For better and more accurate result we should choose it with other methods, but for now we can use this value.
As the main algorithm we take Conjugate Gradient.
This implementation of backpropagation is a little bit different from main interpretation of Conjugate Gradient.
For Backpropagation implementation we can’t guarantee that we get the local minimum in n-th steps (where <cite>n</cite> is the dimention).
To optimize it we should use linear search.
It will fix and set up better steps for Conjugate Gradient.</p>
<p>Now we are going to train the network.
For training we set up 100 epochs.
Also we will add test data into training function to check validation error on every epoch.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">cgnet</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/cgnet-train.png"><img alt="Conjgate Gradient train" src="_images/cgnet-train.png" style="width: 80%;"/></a>
</div>
<p>To make sure that all training processes go in a right way we can check erros updates while the training is in process.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">cgnet</span><span class="o">.</span><span class="n">plot_errors</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/cgnet-error-plot.png"><img alt="Conjgate Gradient train" src="_images/cgnet-error-plot.png" style="width: 80%;"/></a>
</div>
<p>Error minimization procedure looks fine.
The problem is, that last error doesn’t show us the full picture of prediction accuracy.
Our output is always between zero and one and we count the results always into Mean Square Error.
To fix it, we are going to inverse our transformation for predicted and actual values and for accuracy measurment we will use Root Mean Square Logarithmic Error (RMSLE).</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">neuralpy.functions.errors</span> <span class="kn">import</span> <span class="n">rmsle</span>

<span class="n">y_predict</span> <span class="o">=</span> <span class="n">cgnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">rmsle</span><span class="p">(</span><span class="n">target_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span>
              <span class="n">target_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_predict</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we can see that our error equals to <cite>0.2098</cite> which is pretty small.
In the table below you can find 10 randomly chosen errors.</p>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>Actual</th>
      <th>Predicted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>31.2</td>
      <td>27.5</td>
    </tr>
    <tr>
      <td>18.7</td>
      <td>18.5</td>
    </tr>
    <tr>
      <td>20.1</td>
      <td>18.5</td>
    </tr>
    <tr>
      <td>17.2</td>
      <td>9.5</td>
    </tr>
    <tr>
      <td>8.3</td>
      <td>9.5</td>
    </tr>
    <tr>
      <td>50.0</td>
      <td>41.0</td>
    </tr>
    <tr>
      <td>42.8</td>
      <td>32.0</td>
    </tr>
    <tr>
      <td>20.5</td>
      <td>18.5</td>
    </tr>
    <tr>
      <td>16.8</td>
      <td>23.0</td>
    </tr>
    <tr>
      <td>11.8</td>
      <td>9.5</td>
    </tr>
  </tbody>
</table><p>The results are good for the first network implementation.
There are a lot of things which we can do to improve network results, but we will discuss them in an another tutorial.</p>

        </div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="tags/supervised.html">supervised</a>, <a href="tags/backpropagation.html">backpropagation</a>, <a href="tags/regression.html">regression</a></span>
        </div>
        </div><div class="separator post_separator"></div><div class="timestamp postmeta">
            <span>July 04, 2015</span>
        </div>
        <div class="section">
            <h1><a href="2015/07/04/visualize_backpropagation_algorithms.html"><a class="toc-backref" href="#id3">Visualize Backpropagation Algorithms</a></a></h1>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#visualize-backpropagation-algorithms" id="id3">Visualize Backpropagation Algorithms</a><ul>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#checking-data" id="id4">Checking data</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#initialize-contour" id="id5">Initialize contour</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#id1" id="id6">Visualize Backpropagation algorithms</a><ul>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#gradient-descent" id="id7">Gradient Descent</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#momentum" id="id8">Momentum</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#rprop" id="id9">RPROP</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#irprop" id="id10">iRPROP+</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#conjugate-gradient-and-golden-search" id="id11">Conjugate Gradient and Golden Search</a></li>
</ul>
</li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#bring-them-all-together" id="id12">Bring them all together</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#summary" id="id13">Summary</a></li>
</ul>
</li>
</ul>
</div>
<p>In this tutorial we will test different variations of Backpropagation algorithms, visualize them and try to figure out some important features from a plot that we will get.</p>
<div class="section" id="checking-data">
<h2><a class="toc-backref" href="#id4">Checking data</a></h2>
<p>First of all we need to define simple data set which contains 6 points with two features.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
<span class="p">])</span>
<span class="n">target_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
<span class="p">])</span>
</pre></div>
</div>
<p>So we can make a scatter plot and look closer at this dots.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">input_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">input_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">target_data</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/bp-vis-scatter.png"><img alt="Dataset scatter plot" src="_images/bp-vis-scatter.png" style="width: 80%;"/></a>
</div>
<p>From the figure above we can clearly see that all dots are linearly separable and we are able to solve this problem with simple perceptron.
But a goal of this tutorial is to make clear visualization of learning process for different Backpropagation algorithm variations, so the problem must be as simple as possible, because in other cases it will be complex to visualize.</p>
<p>So, as the problem is linear separable we can solve it without hidden layers in network.
There are two features and two classes, so we can build network which will take 2 input values and 1 output.
We need just two weights, so we can visualize them in contour plot.</p>
</div>
<div class="section" id="initialize-contour">
<h2><a class="toc-backref" href="#id5">Initialize contour</a></h2>
<p>Below I will describe several functions that will be needed for visualization later.
For understanding of the entire content of the article it is not necessary to understand all code below.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">neuralpy</span> <span class="kn">import</span> <span class="n">algorithms</span><span class="p">,</span> <span class="n">layers</span>

<span class="c"># Got color from page:</span>
<span class="c"># http://matplotlib.org/examples/pylab_examples/custom_cmap.html</span>
<span class="n">blue_red_cmap</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'red'</span><span class="p">:</span>  <span class="p">((</span><span class="mf">0.00</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span>
             <span class="p">(</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span>
             <span class="p">(</span><span class="mf">0.50</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
             <span class="p">(</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
             <span class="p">(</span><span class="mf">1.00</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)),</span>

    <span class="s">'green'</span><span class="p">:</span> <span class="p">((</span><span class="mf">0.00</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span>
              <span class="p">(</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span>
              <span class="p">(</span><span class="mf">0.50</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">),</span>
              <span class="p">(</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span>
              <span class="p">(</span><span class="mf">1.00</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)),</span>

    <span class="s">'blue'</span><span class="p">:</span>  <span class="p">((</span><span class="mf">0.00</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">),</span>
              <span class="p">(</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
              <span class="p">(</span><span class="mf">0.50</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">),</span>
              <span class="p">(</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span>
              <span class="p">(</span><span class="mf">1.00</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">))</span>
<span class="p">}</span>

<span class="c"># Setup default networks settings which we will use in all algorithms</span>
<span class="c"># which we will vizualize.</span>
<span class="n">network_settings</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">step</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="c"># Disable bias, because we need two parameters for visualization</span>
    <span class="n">use_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">draw_countour</span><span class="p">(</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">ygrid</span><span class="p">,</span> <span class="n">target_function</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">xgrid</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ygrid</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">xgrid</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ygrid</span><span class="p">):</span>
            <span class="n">output</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">target_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">ygrid</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">register_cmap</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'BlueRed'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">blue_red_cmap</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">75</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'BlueRed'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">target_function</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">network</span><span class="o">.</span><span class="n">input_layer</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">x</span><span class="p">],</span> <span class="p">[</span><span class="n">y</span><span class="p">]])</span>
    <span class="n">predicted</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">network</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">predicted</span><span class="p">,</span> <span class="n">target_data</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">prepare_plot</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Approximation function contour plot"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"First weight"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Second weight"</span><span class="p">)</span>

    <span class="n">draw_countour</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
        <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">4.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
        <span class="n">network_target_function</span>
    <span class="p">)</span>

<span class="n">bp_network</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">Backpropagation</span><span class="p">(</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">SigmoidLayer</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">layers</span><span class="o">.</span><span class="n">OutputLayer</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="o">**</span><span class="n">network_settings</span>
<span class="p">)</span>
<span class="n">network_target_function</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">target_function</span><span class="p">,</span> <span class="n">bp_network</span><span class="p">)</span>

<span class="n">prepare_plot</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="_images/raw-contour-plot.png"><img alt="Approximation function contour plot" class="align-center" src="_images/raw-contour-plot.png" style="width: 80%;"/></a>
<p>The plot above shows the approximation error rate depence on the network weights.
The best error must be as small as possible.
The best weights combination for this problem should be near the lower right corner in the blue area.</p>
<p>Next, we are going to look at 5 algorithms based on Backpropagation. They are:</p>
<ul class="simple">
<li>Gradient descent</li>
<li>Momentum</li>
<li>RPROP</li>
<li>iRPROP+</li>
<li>Conjugate Gradient + Golden Search</li>
</ul>
<p>Before starting learning process visualization we should define a few functions which will
give us possibility to use quiver graph as the direction and size of changes in
the weights after an epoch.</p>
<p>Let’s define start point for our algorithms.
As we can see from the figure above the dot (-4, -4) is on the red part of the plot
and the error for it would be approximetly 0.43, so we define default weights on this dot.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">default_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">4.</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">4.</span><span class="p">]])</span>
</pre></div>
</div>
<p>Also we should track weight updates after every epoch and store them in some global variable.
For this option we define a signal which will run our <span class="docutils literal"><span class="pre">save_weight_in_epoch</span></span> function.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">weights</span> <span class="o">=</span> <span class="bp">None</span>

<span class="k">def</span> <span class="nf">save_weight_in_epoch</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">weights</span>
    <span class="n">input_layer_weight</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">train_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">weights</span><span class="p">[:,</span> <span class="n">net</span><span class="o">.</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="n">net</span><span class="o">.</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_layer_weight</span>

<span class="n">network_settings</span><span class="p">[</span><span class="s">'train_epoch_end_signal'</span><span class="p">]</span> <span class="o">=</span> <span class="n">save_weight_in_epoch</span>
</pre></div>
</div>
<p>The next function draws weight update history in a plot.
If you are not familiar with <cite>matplotlib</cite> library quiver, you can skip this code.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">def</span> <span class="nf">weight_quiver</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'c'</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
               <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
               <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
               <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
               <span class="n">scale_units</span><span class="o">=</span><span class="s">'xy'</span><span class="p">,</span> <span class="n">angles</span><span class="o">=</span><span class="s">'xy'</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
</pre></div>
</div>
<p>And the last important function will do the rest of the work.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="kn">as</span> <span class="nn">mpatches</span>

<span class="k">def</span> <span class="nf">draw_quiver</span><span class="p">(</span><span class="n">network_class</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">weights</span>

    <span class="n">input_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">SigmoidLayer</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">default_weight</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
    <span class="n">output_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">OutputLayer</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">bpn</span> <span class="o">=</span> <span class="n">network_class</span><span class="p">(</span>
        <span class="n">input_layer</span> <span class="o">&gt;</span> <span class="n">output_layer</span><span class="p">,</span>
        <span class="o">**</span><span class="n">network_settings</span>
    <span class="p">)</span>
    <span class="c"># 1000 is an upper limit for all network epochs, later we</span>
    <span class="c"># will fix it size</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span>
    <span class="n">weights</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">default_weight</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">bpn</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">target_data</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.125</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[:,</span> <span class="p">:</span><span class="n">bpn</span><span class="o">.</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">weight_quiver</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>

    <span class="n">label</span> <span class="o">=</span> <span class="s">"{name} ({n} steps)"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">bpn</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mpatches</span><span class="o">.</span><span class="n">Patch</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
<p>Function <span class="docutils literal"><span class="pre">draw_quiver</span></span> takes 3 parameters.
First one is the network class which we want to visualize.
Second one is the network name and the third one is the quiver color.</p>
<p>This function will train the network until the error will be smaller than <cite>0.125</cite>.
Path for all networks would be the same.
Every network starts at dot with coordinates <cite>(-4, -4)</cite> and finishes near the point with the closest value to <cite>0.125</cite>.</p>
</div>
<div class="section" id="id1">
<h2><a class="toc-backref" href="#id6">Visualize Backpropagation algorithms</a></h2>
<div class="section" id="gradient-descent">
<h3><a class="toc-backref" href="#id7">Gradient Descent</a></h3>
<p>Let’s primarily check <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.backpropagation.html#neuralpy.algorithms.backprop.backpropagation.Backpropagation" title="neuralpy.algorithms.backprop.backpropagation.Backpropagation"><span class="xref py py-class docutils literal"><span class="pre">Gradient</span> <span class="pre">Descent</span></span></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">prepare_plot</span><span class="p">()</span>
<span class="n">draw_quiver</span><span class="p">(</span><span class="n">algorithms</span><span class="o">.</span><span class="n">Backpropagation</span><span class="p">,</span> <span class="s">'Gradient Descent'</span><span class="p">,</span> <span class="s">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/bp-steps.png"><img alt="Backpropagation steps" src="_images/bp-steps.png" style="width: 80%;"/></a>
</div>
<p>Backpropagation got to the value close to 0.125 using 798 steps and this black curve are just tiny steps of backpropagation algorithm.
We can zoom it and look closer.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/bp-steps-zoom.png"><img alt="Backpropagation steps on zoom" src="_images/bp-steps-zoom.png" style="width: 80%;"/></a>
</div>
<p>Now we can see a lot of information about backpropagation algorithm.
All steps for backpropagation algorithm have approximately similar magnitude.
Their direction doesn’t vary because contours in the zoomed picture are parallel to each other and in it we can see that there is still a lot of steps to achieve the minimum.
Also we can see that small vectors are perpendicular to the contour.</p>
</div>
<div class="section" id="momentum">
<h3><a class="toc-backref" href="#id8">Momentum</a></h3>
<p>Now let’s look at another important algorithm - <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.momentum.html#neuralpy.algorithms.backprop.momentum.Momentum" title="neuralpy.algorithms.backprop.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">prepare_plot</span><span class="p">()</span>
<span class="n">draw_quiver</span><span class="p">(</span><span class="n">algorithms</span><span class="o">.</span><span class="n">Momentum</span><span class="p">,</span> <span class="s">'Momentum'</span><span class="p">,</span> <span class="s">'m'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/momentum-steps.png"><img alt="Momentum steps" src="_images/momentum-steps.png" style="width: 80%;"/></a>
</div>
<p><a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.momentum.html#neuralpy.algorithms.backprop.momentum.Momentum" title="neuralpy.algorithms.backprop.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> got to the value close to 0.125 by 202 steps, which is almost 4 times fewer steps than previously.
The basic idea behind <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.momentum.html#neuralpy.algorithms.backprop.momentum.Momentum" title="neuralpy.algorithms.backprop.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> algorithm, compared to the previous epoch, is that we are strengthening the update if the sign is not changed, and minimize it in another case.</p>
<p>Even if the number of steps fewer than in <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.backpropagation.html#neuralpy.algorithms.backprop.backpropagation.Backpropagation" title="neuralpy.algorithms.backprop.backpropagation.Backpropagation"><span class="xref py py-class docutils literal"><span class="pre">Backpropagation</span></span></a> we still can’t clearly see the updates.
For our eye they are just appeared as a line.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/momentum-steps-zoom.png"><img alt="Momentum steps zoom on increasing weight update size" src="_images/momentum-steps-zoom.png" style="width: 80%;"/></a>
</div>
<p>When we zoom the plot we can see that the direction for weight update vectors is almost the same and gradient sign doesn’t change after each epoch.
At the end of the zoomed plot above vector is bigger than the first one on the same plot.
Update vector hasn’t changed the sign before it’s value, so every next update is the same and it is increased by previous gradient magnitude.
Also we can encounter a situation when weight update size reduces over epochs.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/momentum-steps-zoom-decrease.png"><img alt="Momentum steps zoom on decreasing weight update size" src="_images/momentum-steps-zoom-decrease.png" style="width: 80%;"/></a>
</div>
<p>In plot above we moved to the part where we can see that weight update vector had changed its direction, so the magnitude of this vector had decreased.</p>
</div>
<div class="section" id="rprop">
<h3><a class="toc-backref" href="#id9">RPROP</a></h3>
<p><a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.momentum.html#neuralpy.algorithms.backprop.momentum.Momentum" title="neuralpy.algorithms.backprop.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> makes fewer steps for a prediction, but we still can
find minimum in fewer number of steps.
Now we are going to run <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.rprop.html#neuralpy.algorithms.backprop.rprop.RPROP" title="neuralpy.algorithms.backprop.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> algorithm.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">prepare_plot</span><span class="p">()</span>
<span class="n">draw_quiver</span><span class="p">(</span><span class="n">algorithms</span><span class="o">.</span><span class="n">RPROP</span><span class="p">,</span> <span class="s">'RPROP'</span><span class="p">,</span> <span class="s">'c'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/rprop-steps.png"><img alt="RPROP steps" src="_images/rprop-steps.png" style="width: 80%;"/></a>
</div>
<p>This improvment looks impressive.
Now we are able to see steps without zooming.
We got almost the same value as before using just 19 steps, which is 10 times fewer than <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.momentum.html#neuralpy.algorithms.backprop.momentum.Momentum" title="neuralpy.algorithms.backprop.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> and 40 times fewer than <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.backpropagation.html#neuralpy.algorithms.backprop.backpropagation.Backpropagation" title="neuralpy.algorithms.backprop.backpropagation.Backpropagation"><span class="xref py py-class docutils literal"><span class="pre">Gradient</span> <span class="pre">Descent</span></span></a>.</p>
<p>Now we are going to figure out what are the main features of <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.rprop.html#neuralpy.algorithms.backprop.rprop.RPROP" title="neuralpy.algorithms.backprop.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> just by looking at the plot above.
<a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.rprop.html#neuralpy.algorithms.backprop.rprop.RPROP" title="neuralpy.algorithms.backprop.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> has a unique step for each weight.
There are just two steps for each weight in the input layer for this network.
<a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.rprop.html#neuralpy.algorithms.backprop.rprop.RPROP" title="neuralpy.algorithms.backprop.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> will increase the step size if gradient don’t change the sign before its value, and it will decrease in a different situation.
This update rule is not the same as for <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.momentum.html#neuralpy.algorithms.backprop.momentum.Momentum" title="neuralpy.algorithms.backprop.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> algorithm.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/rprop-first-6-steps.png"><img alt="RPROP first 6 steps" src="_images/rprop-first-6-steps.png" style="width: 80%;"/></a>
</div>
<p>From the figure above you can see that first 6 updates have the same direction, so both steps are increase after each iteration.
For the first epoch steps are equal to the same value which we set up at network initialization step.
On the every next iterations they have been increased by the same factor, so after six iteration they became bigger, but they are still equal because they were getting bigger by the same factor.</p>
<p>Now let’s check the next epochs from the figure below.
On the 7th epoch gradient changed the direction, but steps are still the same.
But we can clearly see that gradient changed the sign for the second weight.
<a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.rprop.html#neuralpy.algorithms.backprop.rprop.RPROP" title="neuralpy.algorithms.backprop.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> updated the step after weight had updated, so the step for the second weight must be fewer for the 8th epoch.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/rprop-6th-to-9th-epochs.png"><img alt="RPROP from 6th to 9th steps" src="_images/rprop-6th-to-9th-epochs.png" style="width: 80%;"/></a>
</div>
<p>Now let’s look at the 8th epoch.
It shows us how gradient sign difference on the 7th epoch updated steps.
Now the steps are not equal.
From the picture above we can see that update on the second weight (y axis) is fewer than on the first weight (x axis).</p>
<p>On the 8th epoch gradient on y axis changed the sign again.
Network decreased by constant factor and update for the second weight on the 9th epoch would be fewer than on the 8th.</p>
<p>To train your intuition you can check the other epochs updates and try to figure out
how steps are dependent on the direction.</p>
</div>
<div class="section" id="irprop">
<h3><a class="toc-backref" href="#id10">iRPROP+</a></h3>
<p><a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.rprop.html#neuralpy.algorithms.backprop.rprop.IRPROPPlus" title="neuralpy.algorithms.backprop.rprop.IRPROPPlus"><span class="xref py py-class docutils literal"><span class="pre">iRPROP+</span></span></a> is almost the same algorithm as <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.rprop.html#neuralpy.algorithms.backprop.rprop.RPROP" title="neuralpy.algorithms.backprop.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> except a small addition.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">prepare_plot</span><span class="p">()</span>
<span class="n">draw_quiver</span><span class="p">(</span><span class="n">algorithms</span><span class="o">.</span><span class="n">IRPROPPlus</span><span class="p">,</span> <span class="s">'iRPROP+'</span><span class="p">,</span> <span class="s">'y'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/irprop-plus-steps.png"><img alt="iRPROP+ steps" src="_images/irprop-plus-steps.png" style="width: 80%;"/></a>
</div>
<p>As in <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.rprop.html#neuralpy.algorithms.backprop.rprop.RPROP" title="neuralpy.algorithms.backprop.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> algorithm <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.rprop.html#neuralpy.algorithms.backprop.rprop.IRPROPPlus" title="neuralpy.algorithms.backprop.rprop.IRPROPPlus"><span class="xref py py-class docutils literal"><span class="pre">iRPROP+</span></span></a> make exacly the
same first 6 steps.</p>
<p>Now let’s look at the 7th step in the figure below.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/irprop-plus-6th-to-12th-epochs.png"><img alt="iRPROP+ from 6th to 12th epoch updates." src="_images/irprop-plus-6th-to-12th-epochs.png" style="width: 80%;"/></a>
</div>
<p>Second weight (on the y axis) didn’t change the value.
On the same epoch <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.rprop.html#neuralpy.algorithms.backprop.rprop.RPROP" title="neuralpy.algorithms.backprop.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> changed the gradient comparing to the previous
epoch and just decreased step value after weight update.
Instead, <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.rprop.html#neuralpy.algorithms.backprop.rprop.IRPROPPlus" title="neuralpy.algorithms.backprop.rprop.IRPROPPlus"><span class="xref py py-class docutils literal"><span class="pre">iRPROP+</span></span></a> just disabled weight update for current
epoch (set it up to <cite>0</cite>).
And of course it also decreased the step for the second weight.
Also you can find that vector for the 7th epoch that looks smaller than for the <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.rprop.html#neuralpy.algorithms.backprop.rprop.RPROP" title="neuralpy.algorithms.backprop.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> algorithm, because we ignored the second weight update.
If we check the x axis update size we will find that it has the same value
as in <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.rprop.html#neuralpy.algorithms.backprop.rprop.RPROP" title="neuralpy.algorithms.backprop.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> algorithm.</p>
<p>On 8th epoch network included again second weight into the update process, because
compared to the previous epoch gradient didn’t change its sign.</p>
<p>Next steps are doing the same job, but 11th epoch differs from others.
There are a few updates which are related specifically to <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.rprop.html#neuralpy.algorithms.backprop.rprop.IRPROPPlus" title="neuralpy.algorithms.backprop.rprop.IRPROPPlus"><span class="xref py py-class docutils literal"><span class="pre">iRPROP+</span></span></a>, but the most important we have not seen before.
After weight update on the 11th epoch network error increased, so our update made our prediction worse.
Now on the 12th epoch network tried to rollback vector update.
It decreased steps on the 11th epoch and weight update didn’t go to the same point after the rollback procedure, it just took opposite direction with a smaller step.</p>
</div>
<div class="section" id="conjugate-gradient-and-golden-search">
<h3><a class="toc-backref" href="#id11">Conjugate Gradient and Golden Search</a></h3>
<p>Now let’s look at <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.conjugate_gradient.html#neuralpy.algorithms.backprop.conjugate_gradient.ConjugateGradient" title="neuralpy.algorithms.backprop.conjugate_gradient.ConjugateGradient"><span class="xref py py-class docutils literal"><span class="pre">Conjugate</span> <span class="pre">Gradient</span></span></a> with
<a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.steps.linear_search.html#neuralpy.algorithms.backprop.steps.linear_search.LinearSearch" title="neuralpy.algorithms.backprop.steps.linear_search.LinearSearch"><span class="xref py py-class docutils literal"><span class="pre">Golden</span> <span class="pre">Search</span></span></a>.
Conjugate Gradient in Backpropagation variation is a little bit different than in
Multivariable Calculus notation and it doesn’t guarantee converge into n-th steps
(<cite>n</cite> means dimmention size for specific problem).
Steps don’t have a perfect size for <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.conjugate_gradient.html#neuralpy.algorithms.backprop.conjugate_gradient.ConjugateGradient" title="neuralpy.algorithms.backprop.conjugate_gradient.ConjugateGradient"><span class="xref py py-class docutils literal"><span class="pre">Conjugate</span> <span class="pre">Gradient</span></span></a>,
so <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.steps.linear_search.html#neuralpy.algorithms.backprop.steps.linear_search.LinearSearch" title="neuralpy.algorithms.backprop.steps.linear_search.LinearSearch"><span class="xref py py-class docutils literal"><span class="pre">Golden</span> <span class="pre">Search</span></span></a> is always a good choice for a step selection.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">prepare_plot</span><span class="p">()</span>
<span class="n">cgnet_golde_search</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">algorithms</span><span class="o">.</span><span class="n">ConjugateGradient</span><span class="p">,</span>
                             <span class="n">optimizations</span><span class="o">=</span><span class="p">[</span><span class="n">algorithms</span><span class="o">.</span><span class="n">LinearSearch</span><span class="p">])</span>
<span class="n">draw_quiver</span><span class="p">(</span><span class="n">cgnet_golde_search</span><span class="p">,</span>
            <span class="s">'Conjugate Gradient + Golden Search'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/conj-grad-and-gold-search-steps.png"><img alt="Conjugate Gradient with Golden Search steps" src="_images/conj-grad-and-gold-search-steps.png" style="width: 80%;"/></a>
</div>
<p>From the figure above we can see almost perfect step for the specific direction.
Each of the fifth steps make a great choice for the step size.
Of course it’s not a great assumption.
Golden Search is just trying to figure out the most perfect step size by using a simple search.
But it doing a greate job.</p>
<p>Finally network made 5 steps, but in plot we can clearly see just 4 of them.
The reason is that the update for the second epoch was really small compared to others.
We can zoom the plot and find this tiny step update.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/conj-grad-small-step.png"><img alt="Conjugate Gradient with Golden Search small second step" src="_images/conj-grad-small-step.png" style="width: 80%;"/></a>
</div>
<p>If we add the same <a class="reference internal" href="apidocs/neuralpy.algorithms.backprop.steps.linear_search.html#neuralpy.algorithms.backprop.steps.linear_search.LinearSearch" title="neuralpy.algorithms.backprop.steps.linear_search.LinearSearch"><span class="xref py py-class docutils literal"><span class="pre">Golden</span> <span class="pre">Search</span></span></a> algorithm to the classic Gradient Descent we will get to the minimum into a few steps as well.</p>
</div>
</div>
<div class="section" id="bring-them-all-together">
<h2><a class="toc-backref" href="#id12">Bring them all together</a></h2>
<div class="highlight-python"><div class="highlight"><pre><span class="n">algorithms</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">(</span><span class="n">algorithms</span><span class="o">.</span><span class="n">Backpropagation</span><span class="p">,</span> <span class="s">'Gradient Descent'</span><span class="p">,</span> <span class="s">'k'</span><span class="p">),</span>
    <span class="p">(</span><span class="n">algorithms</span><span class="o">.</span><span class="n">Momentum</span><span class="p">,</span> <span class="s">'Momentum'</span><span class="p">,</span> <span class="s">'m'</span><span class="p">),</span>
    <span class="p">(</span><span class="n">algorithms</span><span class="o">.</span><span class="n">RPROP</span><span class="p">,</span> <span class="s">'RPROP'</span><span class="p">,</span> <span class="s">'c'</span><span class="p">),</span>
    <span class="p">(</span><span class="n">algorithms</span><span class="o">.</span><span class="n">IRPROPPlus</span><span class="p">,</span> <span class="s">'iRPROP+'</span><span class="p">,</span> <span class="s">'y'</span><span class="p">),</span>
    <span class="p">(</span><span class="n">cgnet_golde_search</span><span class="p">,</span> <span class="s">'Conjugate Gradient + Golden Search'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">prepare_plot</span><span class="p">()</span>
<span class="n">patches</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">network_params</span> <span class="ow">in</span> <span class="n">algorithms</span><span class="p">:</span>
    <span class="n">quiver_patch</span> <span class="o">=</span> <span class="n">draw_quiver</span><span class="p">(</span><span class="o">*</span><span class="n">network_params</span><span class="p">)</span>
    <span class="n">patches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">quiver_patch</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="o">=</span><span class="n">patches</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/all-algorithms-steps.png"><img alt="All algorithms steps" src="_images/all-algorithms-steps.png" style="width: 80%;"/></a>
</div>
</div>
<div class="section" id="summary">
<h2><a class="toc-backref" href="#id13">Summary</a></h2>
<table border="1" class="docutils" id="id2">
<caption><span class="caption-text">Summary</span></caption>
<colgroup>
<col width="50%"/>
<col width="50%"/>
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Algorithm</th>
<th class="head">Number of steps</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Gradient Descent</td>
<td>798</td>
</tr>
<tr class="row-odd"><td>Momentum</td>
<td>202</td>
</tr>
<tr class="row-even"><td>RPROP</td>
<td>19</td>
</tr>
<tr class="row-odd"><td>iRPROP+</td>
<td>17</td>
</tr>
<tr class="row-even"><td>Conjugate Gradient + Golden Search</td>
<td>5</td>
</tr>
</tbody>
</table>
<p>There is no perfect algorithm for neural network that can solve all problems.
All of them have there own pros and cons.
Some of the algorithms can be memory or computationally expensive and you have to choose an algorithm depend on the task which you want to solve.</p>
</div>

        </div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="tags/supervised.html">supervised</a>, <a href="tags/backpropagation.html">backpropagation</a></span>
        </div>
        </div><div class="archive_link">
        <a href="archive.html"> &mdash; Blog Archive &mdash; </a>
    </div></article><aside class="sidebar"><section><div class="widget">
    <h1>Recent Posts</h1>
    <ul><li>
            <a href="2015/07/04/boston_house_prices_dataset.html">Boston house-prices dataset</a>
        </li><li>
            <a href="2015/07/04/visualize_backpropagation_algorithms.html">Visualize Backpropagation Algorithms</a>
        </li></ul>
</div>
</section><section><div class="widget">
    <h1>Cheat sheet</h1>
    <ul>
        <li><a href="docs/algorithms.html#algorithms">Algorithms</a></li>
        <li><a href="docs/algorithms.html#layers">Layers</a></li>
        <li><a href="docs/algorithms.html#error-functions">Error functions</a></li>
    </ul>
</div></section><section><div class="widget">
    <h1>Installation</h1>
    <div class="highligh-bash">
        <div class="highlight">
            <pre>pip install neural-python</pre>
        </div>
    </div>
    <p>Read more in <a href="pages/documentation.html">Documentation</a>.</p>
</div></section><section><div class="widget" id="searchbox" role="search">
    <h1><a href="#searchbox">Search</a></h1>
    <form action="search.html" method="get">
        <input type="text" name="q" />
        <button type="submit"><span class="fa fa-search"></span></button>
    </form>
</div></section></aside></div> <!-- #main --></div> <!-- #main-container -->

        <div class="footer-container" role="contentinfo"><footer class="wrapper">&copy; Copyright 2015, Yurii Shevchuk. Powered by <a href="http://www.tinkerer.me/">Tinkerer</a> and <a href="http://sphinx.pocoo.org/">Sphinx</a>.</footer>
    <a id="fork_me" href="http://github.com/itdxer/neuralpy">
        <img alt="Fork me" src="_static/img/github-fork-green.png" />
    </a></div> <!-- footer-container -->

      </div> <!--! end of #container --><!--[if lt IE 7 ]>
          <script src="//ajax.googleapis.com/ajax/libs/chrome-frame/1.0.3/CFInstall.min.js"></script>
          <script>window.attachEvent('onload',function(){CFInstall.check({mode:'overlay'})})</script>
        <![endif]-->
    </body>
</html>