<!DOCTYPE html><!--[if lt IE 7]>      <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="https://www.facebook.com/2008/fbml" class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="description" content="Artificial Neural Network library implemented in Python">
        <meta name="viewport" content="width=device-width">
        <title>Articles &mdash; NeuPy</title>
            <link rel="stylesheet" href="_static/normalize.css" type="text/css">
            <link rel="stylesheet" href="_static/sphinx.css" type="text/css">
            <link rel="stylesheet" href="_static/main.css" type="text/css">
            <link rel="stylesheet" href="_static/flat.css" type="text/css">
            <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
            <link rel="stylesheet" href="_static/font-awesome.min.css" type="text/css">
        <link rel="shortcut icon" href="_static/favicon.ico" /><!-- Load modernizr and JQuery -->
        <script src="_static/vendor/modernizr-2.6.2.min.js"></script>
        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="_static/vendor/jquery-1.8.2.min.js"><\/script>')</script>
        <script src="_static/plugins.js"></script>
        <script src="_static/main.js"></script>
        <link rel="alternate" type="application/rss+xml" title="RSS" href="rss.html" /><script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.5',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script><script type="text/javascript" src="_static/underscore.js"></script><script type="text/javascript" src="_static/doctools.js"></script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="_static/disqus.js"></script><script type="text/javascript" src="_static/js/google_analytics.js"></script><script type="text/javascript" src="_static/js/script.js"></script><script type="text/javascript" src="_static/js/copybutton.js"></script>

    <script type="text/javascript">
        $(document).ready(function () {
            // Scroll to content if on small screen
            if (screen.width < 480)
            {
                $(document).scrollTop(document.getElementsByTagName("article")[0].offsetTop - 44);
            }
        });
    </script>
<style media="screen" type="text/css">
    #fork_me { display: none; }
    #fork_me img { position: fixed; top: 0; right: 0; border: 0; width: 130px; }
    @media only screen and (min-width: 768px) { #fork_me { display: inline; } }

    .docutils { width: 100%; }
    .docutils td { padding: 10px; }
    .section { word-wrap:break-word; }
    .descname { font-weight: bold; }
    .highlight-python + .figure { margin-top: 20px; }
    .dataframe { text-align: center !important; width: 100%; margin: 10px 0 10px 0; }
    .dataframe td { padding: 5px; }

    .math .gd { color: #000 !important; } /* Generic.Deleted */
    .math .m { color: #000 !important; } /* Literal.Number */
    .math .s { color: #000 !important; } /* Literal.String */
    .math .mf { color: #000 !important; } /* Literal.Number.Float */
    .math .mh { color: #000 !important; } /* Literal.Number.Hex */
    .math .mi { color: #000 !important; } /* Literal.Number.Integer */
    .math .mo { color: #000 !important; } /* Literal.Number.Oct */
    .math .sc { color: #000 !important; } /* Literal.String.Char */
    .math .s2 { color: #000 !important; } /* Literal.String.Double */
    .math .si { color: #000 !important; } /* Literal.String.Interpol */
    .math .sx { color: #000 !important; } /* Literal.String.Other */
    .math .s1 { color: #000 !important; } /* Literal.String.Single */
    .math .ss { color: #000 !important; } /* Literal.String.Symbol */
    .math .il { color: #000 !important; } /* Literal.Number.Integer.Long */
</style></head>
    <body role="document">
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
        <![endif]-->

      <div id="container"><header role="banner">
            <hgroup>
              <h1><a href="pages/home.html">NeuPy</a></h1><h2>Neural Networks in Python</h2></hgroup>
          </header>
      <nav role="navigation">
            <ul><li class="main-nav">
                  <a href="pages/home.html">Home</a>
                </li>
              <li class="main-nav">
                  <a href="#">Articles</a>
                </li>
              <li class="main-nav">
                  <a href="pages/documentation.html">Documentation</a>
                </li>
              <li class="main-nav">
                  <a href="pages/installation.html">Installation</a>
                </li>
              </ul>
          </nav><div class="main-container" role="main"><div class="main wrapper body clearfix"><article><div class="timestamp postmeta">
            <span>September 21, 2015</span>
        </div>
        <div class="section">
            <span id="id1"/><h1><a href="2015/09/21/password_recovery.html"><a class="toc-backref" href="#id2">Password recovery</a><a class="headerlink" href="#password-recovery" title="Permalink to this headline">¶</a></a></h1>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="2015/09/21/password_recovery.html#password-recovery" id="id2">Password recovery</a><ul>
<li><a class="reference internal" href="2015/09/21/password_recovery.html#data-transformation" id="id3">Data transformation</a></li>
<li><a class="reference internal" href="2015/09/21/password_recovery.html#saving-password-into-the-network" id="id4">Saving password into the network</a></li>
<li><a class="reference internal" href="2015/09/21/password_recovery.html#recovering-password-from-the-network" id="id5">Recovering password from the network</a></li>
<li><a class="reference internal" href="2015/09/21/password_recovery.html#test-it-using-monte-carlo" id="id6">Test it using Monte Carlo</a></li>
<li><a class="reference internal" href="2015/09/21/password_recovery.html#possible-problems" id="id7">Possible problems</a></li>
<li><a class="reference internal" href="2015/09/21/password_recovery.html#summary" id="id8">Summary</a></li>
<li><a class="reference internal" href="2015/09/21/password_recovery.html#download-script" id="id9">Download script</a></li>
</ul>
</li>
</ul>
</div>
<p>In this article we are going to build a simple neural network that will recover password from a broken one.
If you aren’t familiar with a <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> algorithm, you can read <a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#discrete-hopfield-network"><span>this article</span></a>.</p>
<p>Before running all experiments, we need to set up <span class="docutils literal"><span class="pre">seed</span></span> parameter to make all results reproducible.
But you can test code without it.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">environment</span>
<span class="n">environment</span><span class="o">.</span><span class="n">reproducible</span><span class="p">()</span>
</pre></div>
</div>
<p>If you can’t reproduce with your version of Python or libraries you can install those ones that were used in this article:</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">neupy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neupy</span><span class="o">.</span><span class="n">__version__</span>
<span class="go">'0.3.0'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">numpy</span><span class="o">.</span><span class="n">__version__</span>
<span class="go">'1.9.2'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">platform</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">platform</span><span class="o">.</span><span class="n">python_version</span><span class="p">()</span>
<span class="go">'3.4.3'</span>
</pre></div>
</div>
<p>Code works with a Python 2.7 as well.</p>
<div class="section" id="data-transformation">
<h2><a class="toc-backref" href="#id3">Data transformation</a><a class="headerlink" href="#data-transformation" title="Permalink to this headline">¶</a></h2>
<p>Before building the network that will save and recover passwords, we should make transformations for input and output data.
But it wouldn’t be enough just to encode it, we should set up a constant length for an input string to make sure that strings will have the same length
Also we should define what string encoding we will use.
For simplicity we will use only ASCII symbols.
So, let’s define a function that transforms a string into a binary list.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">str2bin</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Text can't contains more "</span>
                         <span class="s2">"than {} symbols"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_length</span><span class="p">))</span>

    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">rjust</span><span class="p">(</span><span class="n">max_length</span><span class="p">)</span>

    <span class="n">bits_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
        <span class="n">bits</span> <span class="o">=</span> <span class="nb">bin</span><span class="p">(</span><span class="nb">ord</span><span class="p">(</span><span class="n">symbol</span><span class="p">))</span>
        <span class="c1"># Cut `0b` from the beggining and fill with zeros if they</span>
        <span class="c1"># are missed</span>
        <span class="n">bits</span> <span class="o">=</span> <span class="n">bits</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span><span class="o">.</span><span class="n">zfill</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
        <span class="n">bits_list</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">bits</span><span class="p">))</span>

    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">bits_list</span><span class="p">)</span>
</pre></div>
</div>
<p>Our function takes 2 parameters.
First one is the string that we want to encode.
And second attribute is setting up a constant length for input vector.
If length of the input string is less than <span class="docutils literal"><span class="pre">max_length</span></span> value, then function fills spaces at the beginning of the string.</p>
<p>Let’s check <span class="docutils literal"><span class="pre">str2bin</span></span> function output.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">str2bin</span><span class="p">(</span><span class="s2">"test"</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="go">[0, 0, 1, 0, 0, 0, 0, 0, 0, ... ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">str2bin</span><span class="p">(</span><span class="s2">"test"</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="go">40</span>
</pre></div>
</div>
<p>ASCII encoding uses 8 bits per symbol and we set up 5 symbols per string, so our vector length equals to 40.
From the first output, as you can see, first 8 symbols are equal to <span class="docutils literal"><span class="pre">00100000</span></span>, that is a space value from the ASCII table.</p>
<p>After preforming recovery procedure we will always be getting a binary list.
So before we begin to store data in neural network, we should define another function that transforms a binary list back into a string (which is basically inversed operation to the previous function).</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">chunker</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">position</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">),</span> <span class="n">size</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">sequence</span><span class="p">[</span><span class="n">position</span><span class="p">:</span><span class="n">position</span> <span class="o">+</span> <span class="n">size</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">bin2str</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="n">characters</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">binary_symbol_code</span> <span class="ow">in</span> <span class="n">chunker</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="n">binary_symbol_str</span> <span class="o">=</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">binary_symbol_code</span><span class="p">))</span>
        <span class="n">character</span> <span class="o">=</span> <span class="nb">chr</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">binary_symbol_str</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
        <span class="n">characters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">character</span><span class="p">)</span>
    <span class="k">return</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">characters</span><span class="p">)</span><span class="o">.</span><span class="n">lstrip</span><span class="p">()</span>
</pre></div>
</div>
<p>If we test this function we will get word <span class="docutils literal"><span class="pre">test</span></span> back.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">bin2str</span><span class="p">(</span><span class="n">str2bin</span><span class="p">(</span><span class="s2">"test"</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="go">'test'</span>
</pre></div>
</div>
<p>Pay attention! Function has removed all spaces at the beggining of the string before bringing them back.
We assume that password won’t contain space at the beggining.</p>
</div>
<div class="section" id="saving-password-into-the-network">
<h2><a class="toc-backref" href="#id4">Saving password into the network</a><a class="headerlink" href="#saving-password-into-the-network" title="Permalink to this headline">¶</a></h2>
<p>Now we are ready to save the password into the network.
For this task we are going to define another function that create network and save password inside of it.
Let’s define this function and later we will look at it step by step.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span>

<span class="k">def</span> <span class="nf">save_password</span><span class="p">(</span><span class="n">real_password</span><span class="p">,</span> <span class="n">noise_level</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">noise_level</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"`noise_level` must be equal or greater than 1."</span><span class="p">)</span>

    <span class="n">binary_password</span> <span class="o">=</span> <span class="n">str2bin</span><span class="p">(</span><span class="n">real_password</span><span class="p">)</span>
    <span class="n">bin_password_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">binary_password</span><span class="p">)</span>

    <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">binary_password</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">noise_level</span><span class="p">):</span>
        <span class="c1"># The farther from the 0.5 value the less likely</span>
        <span class="c1"># password recovery</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="n">bin_password_len</span><span class="p">)</span>
        <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>

    <span class="n">dhnet</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">DiscreteHopfieldNetwork</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">'sync'</span><span class="p">)</span>
    <span class="n">dhnet</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">dhnet</span>
</pre></div>
</div>
<p>If you have already read <a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#discrete-hopfield-network"><span>Discrete Hopfield Network article</span></a>, you should know that if we add only one vector into the network we will get it dublicated or with reversed signs through the whole matrix.
To make it a little bit secure we can add some noise into the network.
For this reason we introduce one additional parameter <span class="docutils literal"><span class="pre">noise_level</span></span> into the function.
This parameter controls number of randomly generated binary vectors.
With each iteration using Binomial distribution we generate random binary vector with 55% probability of getting 1 in <cite>noise</cite> vector.
And then we put all the noise vectors and transformed password into one matrix.
And finaly we save all data in the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.</p>
<p>And that’s it.
Function returns trained network for a later usage.</p>
<p>But why do we use random binary vectors instead of the decoded random strings?
The problem is in the similarity between two vectors.
Let’s check two approaches and compare them with a <a class="reference external" href="https://en.wikipedia.org/wiki/Hamming_distance">Hamming distance</a>.
But before starting we should define a function that measures distance between two vectors.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="k">def</span> <span class="nf">hamming_distance</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">):</span>
    <span class="n">left</span><span class="p">,</span> <span class="n">right</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">left</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">right</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">left</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">right</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Shapes must be equal"</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">left</span> <span class="o">!=</span> <span class="n">right</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">generate_password</span><span class="p">(</span><span class="n">min_length</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">symbols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
        <span class="n">string</span><span class="o">.</span><span class="n">ascii_letters</span> <span class="o">+</span>
        <span class="n">string</span><span class="o">.</span><span class="n">digits</span> <span class="o">+</span>
        <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span>
    <span class="p">)</span>
    <span class="n">password_len</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randrange</span><span class="p">(</span><span class="n">min_length</span><span class="p">,</span> <span class="n">max_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">password</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">symbols</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">password_len</span><span class="p">)]</span>
    <span class="k">return</span> <span class="s1">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">password</span><span class="p">)</span>
</pre></div>
</div>
<p>In addition you can see the <span class="docutils literal"><span class="pre">generate_password</span></span> function that we will use for tests.
Let’s check Hamming distance between two randomly generate password vectors.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">hamming_distance</span><span class="p">(</span><span class="n">str2bin</span><span class="p">(</span><span class="n">generate_password</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span>
<span class="gp">... </span>                 <span class="n">str2bin</span><span class="p">(</span><span class="n">generate_password</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)))</span>
<span class="go">70</span>
</pre></div>
</div>
<p>As we can see two randomly generated passwords are very similar to each other (approximetly 70% (<span class="math">\(100 * (240 - 70) / 240\)</span>) of bits are the same).
But If we compare randomly generated password to random binary vector we will see the difference.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">hamming_distance</span><span class="p">(</span><span class="n">str2bin</span><span class="p">(</span><span class="n">generate_password</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span>
<span class="gp">... </span>                 <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">,</span> <span class="mi">240</span><span class="p">))</span>
<span class="go">134</span>
</pre></div>
</div>
<p>Hamming distance is bigger than in the previous example.
A little bit more than 55% of the bits are different.</p>
<p>The greater the difference between them the easier recovery procedure for the input vectors patterns from the network.
For this reason we use randomly generated binary vector instead of random password.</p>
<p>Of course it’s better to save not randomly generated noise vectors but randomly generated passwords converted into binary vectors, cuz if you use wrong input pattern randomly generated password might be recovered instead of the correct one.</p>
</div>
<div class="section" id="recovering-password-from-the-network">
<h2><a class="toc-backref" href="#id5">Recovering password from the network</a><a class="headerlink" href="#recovering-password-from-the-network" title="Permalink to this headline">¶</a></h2>
<p>Now we are going to define the last function which will recover a password from the network.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="n">broken_password</span><span class="p">):</span>
    <span class="n">test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">str2bin</span><span class="p">(</span><span class="n">broken_password</span><span class="p">))</span>
    <span class="n">recovered_password</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">recovered_password</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">recovered_password</span> <span class="o">=</span> <span class="n">recovered_password</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>

    <span class="k">return</span> <span class="n">bin2str</span><span class="p">(</span><span class="n">recovered_password</span><span class="p">)</span>
</pre></div>
</div>
<p>Function takes two parameters.
The first one is network example from which function will recover a password from a broken one.
And the second parameter is a broken password.</p>
<p>Finnaly we can test password recovery from the network.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">my_password</span> <span class="o">=</span> <span class="s2">"$My%Super^Secret*^&amp;Passwd"</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span> <span class="o">=</span> <span class="n">save_password</span><span class="p">(</span><span class="n">my_password</span><span class="p">,</span> <span class="n">noise_level</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="s2">"-My-Super-Secret---Passwd"</span><span class="p">)</span>
<span class="go">'$My%Super^Secret*^&amp;Passwd'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">==</span> <span class="n">my_password</span>
<span class="go">True</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="s2">"-My-Super"</span><span class="p">)</span>
<span class="go">'\x19`\xa0\x04Í\x14#ÛE2er\x1eÛe#2m4jV\x07PqsCwd'</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="s2">"Invalid"</span><span class="p">)</span>
<span class="go">'\x02 \x1d`\x80$Ì\x1c#ÎE¢eò\x0eÛe§:/$ê\x04\x07@5sCu$'</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="s2">"MySuperSecretPasswd"</span><span class="p">)</span>
<span class="go">'$My%Super^Secret*^&amp;Passwd'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">==</span> <span class="n">my_password</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Everithing looks fine.
After multiple times code running you can rarely find a problem.
Network can produce a string which wasn’t taught.
This string can look almost like a password with a few different symbols.
The problem appears when network creates additional local minimum somewhere between input patterns.
We can’t prevent it from running into the local minimum.
For more information about this problem you can check <a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#discrete-hopfield-network"><span>article about Discrete Hopfield Network</span></a>.</p>
</div>
<div class="section" id="test-it-using-monte-carlo">
<h2><a class="toc-backref" href="#id6">Test it using Monte Carlo</a><a class="headerlink" href="#test-it-using-monte-carlo" title="Permalink to this headline">¶</a></h2>
<p>Let’s test our solution with randomly generated passwords.
For this task we can use Monte Carlo experiment.
At each step we create random password and try to recover it from a broken password.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">pprint</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>

<span class="k">def</span> <span class="nf">cutword</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">fromleft</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">fromleft</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">word</span><span class="p">[</span><span class="o">-</span><span class="n">k</span><span class="p">:]</span> <span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="s1">''</span><span class="p">)</span><span class="o">.</span><span class="n">rjust</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">word</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span> <span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="s1">''</span><span class="p">)</span><span class="o">.</span><span class="n">ljust</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">))</span>

<span class="n">n_times</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">cases</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">([</span>
    <span class="p">(</span><span class="s1">'exclude-one'</span><span class="p">,</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">'exclude-quarter'</span><span class="p">,</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">x</span> <span class="o">//</span> <span class="mi">4</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">'exclude-half'</span><span class="p">,</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">'just-one-symbol'</span><span class="p">,</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">'empty-string'</span><span class="p">,</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">0</span><span class="p">)),</span>
<span class="p">])</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="n">cases</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_times</span><span class="p">):</span>
    <span class="n">real_password</span> <span class="o">=</span> <span class="n">generate_password</span><span class="p">(</span><span class="n">min_length</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">casename</span><span class="p">,</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">cases</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">n_letters</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">real_password</span><span class="p">))</span>
        <span class="n">broken_password</span> <span class="o">=</span> <span class="n">cutword</span><span class="p">(</span><span class="n">real_password</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">n_letters</span><span class="p">,</span>
                                  <span class="n">fromleft</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">dhnet</span> <span class="o">=</span> <span class="n">save_password</span><span class="p">(</span><span class="n">real_password</span><span class="p">,</span> <span class="n">noise_level</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
        <span class="n">recovered_password</span> <span class="o">=</span> <span class="n">recover_password</span><span class="p">(</span><span class="n">dhnet</span><span class="p">,</span> <span class="n">broken_password</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">recovered_password</span> <span class="o">!=</span> <span class="n">real_password</span><span class="p">:</span>
            <span class="n">results</span><span class="p">[</span><span class="n">casename</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"Number of fails for each test case:"</span><span class="p">)</span>
<span class="n">pprint</span><span class="o">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></div>
</div>
<p>After sumbmission your output should look the same as the one below (if you followed everything step by step):</p>
<div class="highlight-python"><div class="highlight"><pre><span/>Number of fails for each test case:
{'exclude-one': 11,
 'exclude-quarter': 729,
 'exclude-half': 5823,
 'just-one-symbol': 9998,
 'empty-string': 10000}
</pre></div>
</div>
<p>At this test we catch two situations when the network recovers the password from one symbol, which is not very good.
It really depends on the noise which we stored inside the network.
Randomization can’t give you perfect results.
Sometimes it can recover a password from an empty string, but such situation is also very rare.</p>
<p>In the last test, on each iteration we cut password from the left side and filled other parts with spaces.
Let’s test another approach.
Let’s cut a password from the right side and see what we’ll get:</p>
<div class="highlight-python"><div class="highlight"><pre><span/>Number of fails for each test case:
{'exclude-one': 17,
 'exclude-quarter': 705,
 'exclude-half': 5815,
 'just-one-symbol': 9995,
 'empty-string': 10000}
</pre></div>
</div>
<p>Results look similar to the previous test.</p>
<p>Another interesting test can take place if you randomly replace some symbols with spaces:</p>
<div class="highlight-python"><div class="highlight"><pre><span/>Number of fails for each test case:
{'exclude-one': 14,
 'exclude-quarter': 749,
 'exclude-half': 5760,
 'just-one-symbol': 9998,
 'empty-string': 10000}
</pre></div>
</div>
<p>The result is very similar to the previous two.</p>
<p>And finally, instead of replacing symbols with spaces we can remove symbols without any replacements.
Results do not look good:</p>
<div class="highlight-python"><div class="highlight"><pre><span/>Number of fails for each test case:
{'exclude-one': 3897,
 'exclude-quarter': 9464,
 'exclude-half': 9943,
 'just-one-symbol': 9998,
 'empty-string': 9998}
</pre></div>
</div>
<p>I guess in first case (<span class="docutils literal"><span class="pre">exclude-one</span></span>) we just got lucky and after eliminating one symbol from the end didn’t shift most of the symbols.
So removing symbols is not a very good idea.</p>
<p>All functions that you need for experiments you can find at the <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/memory/password_recovery.py">github</a>.</p>
</div>
<div class="section" id="possible-problems">
<h2><a class="toc-backref" href="#id7">Possible problems</a><a class="headerlink" href="#possible-problems" title="Permalink to this headline">¶</a></h2>
<p>There are a few possible problems in the Discrete Hopfile Network.</p>
<ol class="arabic simple">
<li>As we saw from the last experiments, shifted passwords are harder to recover than the passwords with missed symbols. It’s better to replace missed symbols with some other things.</li>
<li>There already exists small probability for recovering passwords from empty strings.</li>
<li>Similar binary code representation for different symbols is a big problem. Sometimes you can have a situation where two symbols in binary code represantation are different just by one bit. The first solution is to use a One Hot Encoder. But it can give us even more problems. For example, we used symbols from list of 94 symbols for the password. If we encode each symbol we will get a vector with 93 zeros and just one active value. The problem is that after the recovery procedure we should always get 1 active value, but this situation is very unlikely to happen.</li>
</ol>
</div>
<div class="section" id="summary">
<h2><a class="toc-backref" href="#id8">Summary</a><a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>Despite some problems, network recovers passwords very well.
Monte Carlo experiment shows that the fewer symbols we know the less is probability for recovering them correctly.</p>
<p>Even this simple network can be a powerful tool if you know its limitations.</p>
</div>
<div class="section" id="download-script">
<h2><a class="toc-backref" href="#id9">Download script</a><a class="headerlink" href="#download-script" title="Permalink to this headline">¶</a></h2>
<p>You can download and test a full script from the <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/memory/password_recovery.py">github repository</a>.</p>
<p>It doesn’t contain a fixed <span class="docutils literal"><span class="pre">environment.reproducible</span></span> function, so you will get different outputs after each run.</p>
</div>

        </div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="tags/memory.html">memory</a>, <a href="tags/unsupervised.html">unsupervised</a></span>
        </div>
        <div class="comments">
            <a href="http://neupy.com/2015/09/21/password_recovery.html#disqus_thread" data-disqus-identifier="2015/09/21/password_recovery">Leave a comment</a>
        </div></div><div class="separator post_separator"></div><div class="timestamp postmeta">
            <span>September 20, 2015</span>
        </div>
        <div class="section">
            <span id="discrete-hopfield-network"/><h1><a href="2015/09/20/discrete_hopfield_network.html"><a class="toc-backref" href="#id5">Discrete Hopfiel Network</a><a class="headerlink" href="#discrete-hopfiel-network" title="Permalink to this headline">¶</a></a></h1>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#discrete-hopfiel-network" id="id5">Discrete Hopfiel Network</a><ul>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#architecture" id="id6">Architecture</a><ul>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#training-procedure" id="id7">Training procedure</a></li>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#recovery-from-memory" id="id8">Recovery from memory</a><ul>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#synchronous" id="id9">Synchronous</a></li>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#asynchronous" id="id10">Asynchronous</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#memory-limit" id="id11">Memory limit</a></li>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#hallucinations" id="id12">Hallucinations</a></li>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#example" id="id13">Example</a></li>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#more-reading" id="id14">More reading</a></li>
<li><a class="reference internal" href="2015/09/20/discrete_hopfield_network.html#references" id="id15">References</a></li>
</ul>
</li>
</ul>
</div>
<p>In this article we are going to learn about <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> algorithm.</p>
<p><a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> is a type of algorithms which is called - <a class="reference external" href="https://en.wikipedia.org/wiki/Autoassociative_memory">Autoassociative memories</a>
Don’t be scared of the word <cite>Autoassociative</cite>.
The idea behind this type of algorithms is very simple.
It can store useful information in <cite>memory</cite> and later it is able to reproduce this information from partialy broken patterns.
You can preceive it as human memory.
For instance, imagine that you look at an old picture of a place where you were long time ago, but this picture is of very bad quality and very blurry.
By looking at the picture you manage to recognize a few objects or places that make sense to you and form some objects even though they are blurry.
It can be a house, a lake or anything that can add up to the whole picture and bring out some associations about this place.
With these details that you got from your memory so far other parts of picture start to make even more sense.
Though you don’t clearly see all objects in the picture, you start to remember things and withdraw from your memory some images, that cannot be seen in the picture, just because of those very familiarly-shaped details that you’ve got so far.
That’s what it is all about.
Autoassociative memory networks is a posibily to interprete functions of memory into neural network model.</p>
<p>Don’t worry if you have only basic knowledge in Linear Algebra; in this article I’ll try to explain the idea as simple as possible.
If you are interested in proofs of the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> you can check them at R. Rojas. Neural Networks <a class="footnote-reference" href="#id2" id="id1">[1]</a> book.</p>
<div class="section" id="architecture">
<h2><a class="toc-backref" href="#id6">Architecture</a><a class="headerlink" href="#architecture" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> is an easy algorithm.
It’s simple because you don’t need a lot of background knowledge in Maths for using it.
Everything you need to know is how to make a basic Linear Algebra operations, like outer product or sum of two matrices.</p>
<p>Let’s begin with a basic thing.
What do we know about this neural network so far?
Just the name and the type.
From the name we can identify one useful thing about the network.
It’s <cite>Discrete</cite>.
It means that network only works with binary vectors.
But for this network we wouldn’t use binary numbers in a typical form.
Instead, we will use bipolar numbers.
They are almost the same, but instead of 0 we are going to use -1 to decode a negative state.
We can’t use zeros.
And there are two main reasons for it.
The first one is that zeros reduce information from the network weight, later in this article you are going to see it.
The second one is more complex, it depends on the nature of bipolar vectors.
Basically they are more likely to be orthogonal to each other which is a critical moment for the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.
But as I mentioned before we won’t talk about proofs or anything not related to basic understanding of Linear Algebra operations.</p>
<p>So, let’s look at how we can train and use the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.</p>
<div class="section" id="training-procedure">
<h3><a class="toc-backref" href="#id7">Training procedure</a><a class="headerlink" href="#training-procedure" title="Permalink to this headline">¶</a></h3>
<p>We can’t use memory without any patterns stored in it.
So first of all we are going to learn how to train the network.
For the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> train procedure doesn’t require any iterations.
It includes just an outer product between input vector and transposed input vector.</p>
<div class="math">
\[\begin{split}\begin{align*}
    W = x \cdot x^T =
    \left[
    \begin{array}{c}
      x_1\\
      x_2\\
      \vdots\\
      x_n
    \end{array}
    \right]
    \cdot
    \left[
    \begin{array}{c}
      x_1 &amp; x_2 &amp; \cdots &amp; x_n
    \end{array}
    \right]
\end{align*}
=\end{split}\]\[\begin{split}\begin{align*}
    =
    \left[
    \begin{array}{c}
      x_1^2 &amp; x_1 x_2 &amp; \cdots &amp; x_1 x_n \\
      x_2 x_1 &amp; x_2^2 &amp; \cdots &amp; x_2 x_n \\
      \vdots\\
      x_n x_1 &amp; x_n x_2 &amp; \cdots &amp; x_n^2 \\
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p><span class="math">\(W\)</span> is a weight matrix and <span class="math">\(x\)</span> is an input vector.
Each value <span class="math">\(x_i\)</span> in the input vector can only be -1 or 1.
So on the matrix diagonal we only have squared values and it means we will always see 1s at those places.
Think about it, everytime, in any case, values on the diagonal can take just one possible state.
We can’t use this information, because it doesn’t say anything useful about patterns that are stored in the memory and even can make incorrect contribution into the output result.
For this reason we need to set up all the diagonal values equal to zero.
The final weight formula should look like this one below.</p>
<div class="math">
\[\begin{split}\begin{align*}
    W =
    x x^T - I =
    \left[
    \begin{array}{c}
      0 &amp; x_1 x_2 &amp; \cdots &amp; x_1 x_n \\
      x_2 x_1 &amp; 0 &amp; \cdots &amp; x_2 x_n \\
      \vdots\\
      x_n x_1 &amp; x_n x_2 &amp; \cdots &amp; 0 \\
    \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>Where <span class="math">\(I\)</span> is an identity matrix.</p>
<p>But usualy we need to store more values in memory.
For another pattern we have to do exacly the same procedure as before and then just add the generated weight matrix to the old one.</p>
<div class="math">
\[W = W_{old} + W_{new}\]</div>
<p>And this procedure generates us a new weight that would be valid for both previously stored patterns.
Later you can add other patterns using the same algorithm.</p>
<p>But if you need to store multiple vectors inside the network at the same time you don’t need to compute the weight for each vector and then sum them up.
If you have a matrix <span class="math">\(X \in \Bbb R^{m\times n}\)</span> where each row is the input vector, then you can just make product matrix between transposed input matrix and input matrix.</p>
<div class="math">
\[W = X^T X - m I\]</div>
<p>Where <span class="math">\(I\)</span> is an identity matrix (<span class="math">\(I \in \Bbb R^{n\times n}\)</span>), <span class="math">\(n\)</span> is a number of features in the input vector and <span class="math">\(m\)</span> is a number of input patterns inside the matrix <span class="math">\(X\)</span>.
Term <span class="math">\(m I\)</span> removes all values from the diagonal.
Basically we remove 1s for each stored pattern and since we have <span class="math">\(m\)</span> of them, we should do it <span class="math">\(m\)</span> times.
Practically, it’s not very good to create an identity matrix just to set up zeros on the diagonal, especially when dimension on the matrix is very big.
Usually linear algebra libraries give you a possibility to set up diagonal values without creating an additional matrix and this solution would be more efficient.
For example in NumPy library it’s a <a class="reference external" href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.fill_diagonal.html">numpy.fill_diagonal</a> function.</p>
<p>Let’s check an example just to make sure that everything is clear.
Let’s pretend we have a vector <span class="math">\(u\)</span>.</p>
<div class="math">
\[\begin{split}u = \left[\begin{align*}1 \\ -1 \\ 1 \\ -1\end{align*}\right]\end{split}\]</div>
<p>Assume that network doesn’t have patterns inside of it, so the vector <span class="math">\(u\)</span> would be the first one.
Let’s compute weights for the network.</p>
<div class="math">
\[\begin{split}\begin{align*}
    U = u u^T =
    \left[
        \begin{array}{c}
            1 \\
            -1 \\
            1 \\
            -1
        \end{array}
    \right]
    \left[
        \begin{array}{c}
            1 &amp; -1 &amp; 1 &amp; -1
        \end{array}
    \right]
    =
    \left[
        \begin{array}{cccc}
            1 &amp; -1 &amp; 1 &amp; -1\\
            -1 &amp; 1 &amp; -1 &amp; 1\\
            1 &amp; -1 &amp; 1 &amp; -1\\
            -1 &amp; 1 &amp; -1 &amp; 1
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>Look closer to the matrix <span class="math">\(U\)</span> that we got.
Outer product just repeats vector 4 times with the same or inversed values.
First and third columns (or rows, it doesn’t matter, because matrix is symmetrical) are exacly the same as the input vector.
The second and fourth are also the same, but with an opposite sign.
That’s because in the vector <span class="math">\(u\)</span> we have 1 on the first and third places and -1 on the other.</p>
<p>To make weight from the <span class="math">\(U\)</span> matrix, we need to remove ones from the diagonal.</p>
<div class="math">
\[W = U - I =\]\[\begin{split}= \left[
    \begin{array}{cccc}
        1 &amp; -1 &amp; 1 &amp; -1\\
        -1 &amp; 1 &amp; -1 &amp; 1\\
        1 &amp; -1 &amp; 1 &amp; -1\\
        -1 &amp; 1 &amp; -1 &amp; 1
    \end{array}
\right] -
\left[
    \begin{array}{cccc}
        1 &amp; 0 &amp; 0 &amp; 0\\
        0 &amp; 1 &amp; 0 &amp; 0\\
        0 &amp; 0 &amp; 1 &amp; 0\\
        0 &amp; 0 &amp; 0 &amp; 1
    \end{array}
\right] =\end{split}\]\[\begin{split}= \left[
    \begin{array}{cccc}
        0 &amp; -1 &amp; 1 &amp; -1\\
        -1 &amp; 0 &amp; -1 &amp; 1\\
        1 &amp; -1 &amp; 0 &amp; -1\\
        -1 &amp; 1 &amp; -1 &amp; 0
    \end{array}
\right]\end{split}\]</div>
<p><span class="math">\(I\)</span> is the identity matrix and <span class="math">\(I \in \Bbb R^{n \times n}\)</span>, where <span class="math">\(n\)</span> is a number of features in the input vector.</p>
<p>When we have one stored vector inside the weights we don’t really need to remove 1s from the diagonal.
The main problem would appear when we have more than one vector stored in the weights.
Each value on the diagonal would be equal to the number of stored vectors in it.</p>
</div>
<div class="section" id="recovery-from-memory">
<h3><a class="toc-backref" href="#id8">Recovery from memory</a><a class="headerlink" href="#recovery-from-memory" title="Permalink to this headline">¶</a></h3>
<p>The main advantage of Autoassociative network is that it is able to recover pattern from the memory using just a partial information about the pattern.
There are already two main approaches to this situation, synchronous and asynchronous.
We are going to master both of them.</p>
<div class="section" id="synchronous">
<h4><a class="toc-backref" href="#id9">Synchronous</a><a class="headerlink" href="#synchronous" title="Permalink to this headline">¶</a></h4>
<p>Synchronous approach is much more easier for understanding, so we are going to look at it firstly.
To recover your pattern from memory you just need to multiply the weight matrix by the input vector.</p>
<div class="math">
\[\begin{split}\begin{align*}
    s = {W}\cdot{x}=
    \left[
    \begin{array}{cccc}
      w_{11} &amp; w_{12} &amp; \ldots &amp; w_{1n}\\
      w_{21} &amp; w_{22} &amp; \ldots &amp; w_{2n}\\
      \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
      w_{n1} &amp; w_{n2} &amp; \ldots &amp; w_{nn}
    \end{array}
    \right]
    \left[
    \begin{array}{c}
      x_1\\
      x_2\\
      \vdots\\
      x_n
    \end{array}
    \right]
    =
\end{align*}\end{split}\]\[\begin{split}\begin{align*}
    =
    \left[
        \begin{array}{c}
          w_{11}x_1+w_{12}x_2 + \cdots + w_{1n} x_n\\
          w_{21}x_1+w_{22}x_2 + \cdots + w_{2n} x_n\\
          \vdots\\
          w_{n1}x_1+w_{n2}x_2 + \cdots + w_{nn} x_n\\
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>Let’s analyze the result.
We summed up all information from the weights where each value can be any integer with an absolute value equal to or smaller than the number of patterns inside the network.
It’s clear that total sum value for <span class="math">\(s_i\)</span> is not necessary equal to -1 or 1, so we have to make additional operations that will make bipolar vector from the vector <span class="math">\(s\)</span>.</p>
<p>Let’s think about this product operation.
What does it actualy do?
Basically after training procedure we saved our pattern dublicated <span class="math">\(n\)</span> times (where <span class="math">\(n\)</span> is a number of input vector features) inside the weight.
When we store more patterns we get interception between them (it’s called a <strong>crosstalk</strong>) and each pattern add some noise to other patterns.
So, after perfoming product matrix between <span class="math">\(W\)</span> and <span class="math">\(x\)</span> for each value from the vector <span class="math">\(x\)</span> we’ll get a recovered vector with a little bit of noise.
For <span class="math">\(x_1\)</span> we get a first column from the matrix <span class="math">\(W\)</span>, for the <span class="math">\(x_2\)</span> a second column, and so on.
Then we sum up all vectors together.
This operation can remind you of voting.
For example we have 3 vectors.
If the first two vectors have 1 in the first position and the third one has -1 at the same position, the winner should be 1.
We can perform the same procedure with <span class="math">\(sign\)</span> function.
So the output value should be 1 if total value is greater then zero and -1 otherwise.</p>
<div class="math">
\[\begin{split}sign(x) = \left\{
    \begin{array}{lr}
        &amp;1 &amp;&amp; : x \ge 0\\
        &amp;-1 &amp;&amp; : x &lt; 0
    \end{array}
\right.\\\end{split}\]\[y = sign(s)\]</div>
<p>That’s it.
Now <span class="math">\(y\)</span> store the recovered pattern from the input vector <span class="math">\(x\)</span>.</p>
<p>Maybe now you can see why we can’t use zeros in the input vectors.
In <cite>voting</cite> procedure we use each row that was multiplied by bipolar number, but if values had been zeros they would have ignored columns from the weight matrix and we would have used only values related to ones in the input pattern.</p>
<p>Of course you can use 0 and 1 values and sometime you will get the correct result, but this approach give you much worse results than explained above.</p>
</div>
<div class="section" id="asynchronous">
<h4><a class="toc-backref" href="#id10">Asynchronous</a><a class="headerlink" href="#asynchronous" title="Permalink to this headline">¶</a></h4>
<p>Previous approach is good, but it has some limitations.
If you change one value in the input vector it can change your output result and value won’t converge to the known pattern.
Another popular approach is an <strong>asynchronous</strong>.
This approach is more likely to remind you of real memory.
At the same time in network activates just one random neuron instead of all of them.
In terms of neural networks we say that <strong>neuron fires</strong>.
We iteratively repeat this operation multiple times and after some point network will converge to some pattern.</p>
<p>Let’s look at this example:
Consider that we already have a weight matrix <span class="math">\(W\)</span> with one pattern <span class="math">\(x\)</span>  inside of it.</p>
<div class="math">
\[\begin{split}\begin{align*}
    W =
    \left[
    \begin{array}{cccc}
      0 &amp; 1 &amp; -1 \\
      1 &amp; 0 &amp; -1 \\
      -1 &amp; -1 &amp; 0
    \end{array}
    \right]
\end{align*}\end{split}\]\[\begin{split}\begin{align*}
    x =
    \left[
        \begin{array}{c}
          1\\
          1\\
          -1
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>Let’s assume that we have a vector <span class="math">\(x^{'}\)</span> from which we want to recover the pattern.</p>
<div class="math">
\[\begin{split}\begin{align*}
    x^{'} =
    \left[
        \begin{array}{c}
          1\\
          -1\\
          -1
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>In first iteration one neuron fires.
Let it be the second one.
So we multiply the first column by this selected value.</p>
<div class="math">
\[\begin{split}\begin{align*}
    x^{'}_2 =
    sign(\left[
        \begin{array}{c}
          1 &amp; -1 &amp; -1
        \end{array}
    \right] \cdot \left[
        \begin{array}{c}
          1\\
          0\\
          -1
        \end{array}
    \right]) = sign(2) = 1
\end{align*}\end{split}\]</div>
<p>And after this operation we set up a new value into the input vector <span class="math">\(x\)</span>.</p>
<div class="math">
\[\begin{split}\begin{align*}
    x^{'} =
    \left[
        \begin{array}{c}
          1\\
          1\\
          -1
        \end{array}
    \right]
\end{align*}\end{split}\]</div>
<p>As you can see after first iteration value is exacly the same as <span class="math">\(x\)</span> but we can keep going.
In second iteration random neuron fires again.
Let’s pretend that this time it was the third neuron.</p>
<div class="math">
\[\begin{split}\begin{align*}
    x^{'}_3 =
    sign(\left[
        \begin{array}{c}
          1 &amp; 1 &amp; -1
        \end{array}
    \right] \cdot \left[
        \begin{array}{c}
          -1\\
          -1\\
          0
        \end{array}
    \right]) = sign(-2) = -1
\end{align*}\end{split}\]</div>
<p><span class="math">\(x^{'}_3\)</span> is exacly the same as in the <span class="math">\(x^{'}\)</span> vector so we don’t need to update it.
We can repeat it as many times as we want, but we will be getting the same value.</p>
</div>
</div>
</div>
<div class="section" id="memory-limit">
<h2><a class="toc-backref" href="#id11">Memory limit</a><a class="headerlink" href="#memory-limit" title="Permalink to this headline">¶</a></h2>
<p>Obviously, you can’t store infinite number of vectors inside the network.
There are two good rules of thumb.</p>
<p>Concider that <span class="math">\(n\)</span> is the dimension (number of features) of your input vector and <span class="math">\(m\)</span> is the number of patterns that you want to store in the network.
The first rule gives us a simple ration between <span class="math">\(m\)</span> and <span class="math">\(n\)</span>.</p>
<div class="math">
\[m \approx 0.18 n\]</div>
<p>The main problem with this rule is that proof assumes that stored vectors inside the weight are completly random with an equal probability.
Unfortunately, that is not always true.
Let’s suppose we save some images of numbers from 0 to 9.
Pictures are black and white, so we can encode them in bipolar vectors.
Will the probabilities be the same for seeing as many white pixels as black ones?
Usually no.
More likely that number of white pixels would be greater than number of black ones.
Before use this rule you have to think about type of your input patterns.</p>
<p>The second rule uses a logarithmic proportion.</p>
<div class="math">
\[m = \left \lfloor \frac{n}{2 \cdot log(n)} \right \rfloor\]</div>
<p>Both of these rules are good assumtions about the nature of data and its possible limits in memory.
Of course you can find situations when these rules will fail.</p>
</div>
<div class="section" id="hallucinations">
<h2><a class="toc-backref" href="#id12">Hallucinations</a><a class="headerlink" href="#hallucinations" title="Permalink to this headline">¶</a></h2>
<p>Hallucinations is one of the main problems in the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.
Sometimes network output can be something that we hasn’t taught it.</p>
<p>To understand this phenomena we should firstly define the Hopfield energy function.</p>
<div class="math">
\[E = -\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} w_{ij} x_i x_j + \sum_{i=1}^{n} \theta_i x_i\]</div>
<p>Where <span class="math">\(w_{ij}\)</span> is a weight value on the <span class="math">\(i\)</span>-th row and <span class="math">\(j\)</span>-th column.
<span class="math">\(x_i\)</span> is a <span class="math">\(i\)</span>-th values from the input vector <span class="math">\(x\)</span>.
<span class="math">\(\theta\)</span> is a threshold.
Threshold defines the bound to the sign function.
For this reason <span class="math">\(\theta\)</span> is equal to 0 for the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.
In terms of a linear algebra we can write formula for the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a> energy Function more simplier.</p>
<div class="math">
\[E = -\frac{1}{2} x^T W x\]</div>
<p>But linear algebra notation works only with the <span class="math">\(x\)</span> vector, we can’t use matrix <span class="math">\(X\)</span> with multiple input patterns instead of the <span class="math">\(x\)</span> in this formula.
For the energy function we’re always interested in finding a minimum value, for this reason it has minus sign at the beggining.</p>
<p>Let’s try to visualize it.
Assume that values for vector <span class="math">\(x\)</span> can be continous in order and we can visualize them using two parameters.
Let’s pretend that we have two vectors <cite>[1, -1]</cite> and <cite>[-1, 1]</cite> stored inside the network.
Below you can see the plot that visualizes energy function for this situation.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/energy-function.png"><img alt="Energy function visualization for the network with two neurons" src="_images/energy-function.png" style="width: 80%;"/></a>
</div>
<p>As you can see we have two minimum values at the same points as those patterns that are already stored inside the network.
But between these two patterns function creates a saddle point somewhere at the point with coordinates <span class="math">\((0, 0)\)</span>.
In this case we can’t stick to the points <span class="math">\((0, 0)\)</span>.
But in situation with more dimensions this saddle points can be at the level of available values and they could be hallucination.
Unfortunately, we are very limited in terms of numbers of dimensions we could plot, but the problem is still the same.</p>
<p>Full source code for this plot you can find on <a class="reference external" href="https://github.com/itdxer/neupy/tree/master/examples/memory/dhn_energy_func.py">github</a></p>
</div>
<div class="section" id="example">
<h2><a class="toc-backref" href="#id13">Example</a><a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h2>
<p>Now we are ready for a more practical example.
Let’s define a few images that we are going to teach the network.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">draw_bin_image</span><span class="p">(</span><span class="n">image_matrix</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">image_matrix</span><span class="o">.</span><span class="n">tolist</span><span class="p">():</span>
<span class="gp">... </span>        <span class="k">print</span><span class="p">(</span><span class="s1">'| '</span> <span class="o">+</span> <span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">' *'</span><span class="p">[</span><span class="n">val</span><span class="p">]</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">row</span><span class="p">))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">zero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span>
<span class="gp">... </span><span class="p">])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">one</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="gp">... </span><span class="p">])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">two</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span><span class="p">])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">zero</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * * *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">|   * * *</span>
</pre></div>
</div>
<p>We have 3 images, so now we can train network with these patterns.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">zero</span><span class="p">,</span> <span class="n">one</span><span class="p">,</span> <span class="n">two</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">DiscreteHopfieldNetwork</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">'sync'</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>That’s all.
Now to make sure that network has memorized patterns right we can define the broken patterns and check how the network will recover them.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">half_zero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">half_zero</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * * *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">|</span>
<span class="go">|</span>
<span class="go">|</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">half_two</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">half_two</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|</span>
<span class="go">|</span>
<span class="go">|</span>
<span class="go">|   * *</span>
<span class="go">| *</span>
<span class="go">| * * * * *</span>
</pre></div>
</div>
<p>Now we can reconstruct pattern from the memory.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_zero</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * * *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">| *       *</span>
<span class="go">|   * * *</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_two</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">| * * *</span>
<span class="go">|       *</span>
<span class="go">|       *</span>
<span class="go">|   * *</span>
<span class="go">| *</span>
<span class="go">| * * * * *</span>
</pre></div>
</div>
<p>Cool!
Network catches the pattern right.</p>
<p>But not always we will get the correct answer.
Let’s define another broken pattern and check network output.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="n">half_two</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matrix</span><span class="p">([</span>
<span class="gp">... </span>    <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span><span class="p">])</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_two</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="go">|   * *</span>
<span class="go">| *   *</span>
<span class="go">| * * * * *</span>
</pre></div>
</div>
<p>We hasn’t clearly taught the network to deal with such pattern.
But if we look closer, it looks like mixed pattern of numbers 1 and 2.</p>
<p>This problem we can solve using the asynchronous network approach.
We don’t necessary need to create a new network, we can just simply switch its mode.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">environment</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">environment</span><span class="o">.</span><span class="n">reproducible</span><span class="p">()</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="s1">'async'</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dhnet</span><span class="o">.</span><span class="n">n_times</span> <span class="o">=</span> <span class="mi">400</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_two</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">|   * *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="go">|     *</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dhnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">half_two</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">draw_bin_image</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="go">| * * *</span>
<span class="go">|       *</span>
<span class="go">|       *</span>
<span class="go">|   * *</span>
<span class="go">| *</span>
<span class="go">| * * * *</span>
</pre></div>
</div>
<p>Our broken pattern is really close to the minimum of 1 and 2 patterns.
Randomization helps us choose direction but it’s not nessesary the right one, especialy when the broken pattern is close to 1 and 2 at the same time.</p>
<p>Check last output with number two again.
Is that a realy valid pattern for number 2?
Final symbol in output is wrong.
We are not able to recover patter 2 from this network, because input vector is always much closer to the minimum that looks very similar to pattern 2.</p>
<p>In plot below you can see first 200 iterations of the recovery procedure.
Energy value was decreasing after each iteration until it reached the local minimum where pattern is equal to 2.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/hopfield-energy-vis.png"><img alt="Asynchronous Discrete Hopfield Network energy update after each iteration" src="_images/hopfield-energy-vis.png" style="width: 80%;"/></a>
</div>
<p>And finally we can look closer to the network memory using Hinton diagram.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">plots</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Hinton diagram"</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plots</span><span class="o">.</span><span class="n">hinton</span><span class="p">(</span><span class="n">dhnet</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/hinton-diagram.png"><img alt="Asynchronous Discrete Hopfield Network energy update after each iteration" src="_images/hinton-diagram.png" style="width: 80%;"/></a>
</div>
<p>This graph above shows the network weight matrix and all information stored inside of it.
Hinton diagram is a very simple technique for the weight visualization in neural networks.
Each value encoded in square where its size is an absolute value from the weight matrix and color shows the sign of this value.
White is a positive and black is a negative.
Usualy Hinton diagram helps identify some patterns in the weight matrix.</p>
<p>Let’s go back to the graph.
What can you say about the network just by looking at this picture?
First of all you can see that there is no squares on the diagonal.
That is because they are equal to zero.
The second important thing you can notice is that the plot is symmetrical.
But that is not all that you can withdraw from the graph.
Can you see different patterns?
You can find rows or columns with exacly the same values, like the second and third columns.
Fifth column is also the same but its sign is reversed.
Now look closer to the antidiagonal.
What can you say about it?
If you are thinking that all squares are white - you are right.
But why is that true?
Is there always the same patterns for each memory matrix?
No, it is a special property of patterns that we stored inside of it.
If you draw a horizontal line in the middle of each image and look at it you will see that values are opposite symmetric.
For instance, <span class="math">\(x_1\)</span> opposite symmetric to <span class="math">\(x_{30}\)</span>, <span class="math">\(x_2\)</span> to <span class="math">\(x_{29}\)</span>, <span class="math">\(x_3\)</span> to <span class="math">\(x_{28}\)</span> and so on.
Zero pattern is a perfect example where each value have exacly the same opposite symmetric pair.
One is almost perfect except one value on the <span class="math">\(x_2\)</span> position.
Two is not clearly opposite symmetric.
But if you check each value you will find that more than half of values are symmetrical.
Combination of those patterns gives us a diagonal with all positive values.
If we have all perfectly opposite symmetric patterns then squares on the antidiagonal will have the same length, but in this case pattern for number 2 gives a little bit of noise and squares have different sizes.</p>
<p>Properties that we’ve reviewed so far are just the most interesting and maybe other patterns you can encounter on your own.</p>
</div>
<div class="section" id="more-reading">
<h2><a class="toc-backref" href="#id14">More reading</a><a class="headerlink" href="#more-reading" title="Permalink to this headline">¶</a></h2>
<p>In addition you can read another article about a ‘<a class="reference internal" href="2015/09/21/password_recovery.html#password-recovery"><span>Password recovery</span></a>‘ from the memory using the <a class="reference internal" href="apidocs/neupy.algorithms.memory.discrete_hopfield_network.html#neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork" title="neupy.algorithms.memory.discrete_hopfield_network.DiscreteHopfieldNetwork"><span class="xref py py-class docutils literal"><span class="pre">Discrete</span> <span class="pre">Hopfield</span> <span class="pre">Network</span></span></a>.</p>
</div>
<div class="section" id="references">
<h2><a class="toc-backref" href="#id15">References</a><a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>R. Rojas. Neural Networks. In Associative Networks. pp. 311 - 336, 1996.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Math4IQB. (2013, November 17). Hopfield Networks. Retrieved
from <a class="reference external" href="https://www.youtube.com/watch?v=gfPUWwBkXZY">https://www.youtube.com/watch?v=gfPUWwBkXZY</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label"/><col/></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>R. Callan. The Essence of Neural Networks. In Pattern Association. pp. 84 - 98, 1999.</td></tr>
</tbody>
</table>
</div>

        </div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="tags/memory.html">memory</a>, <a href="tags/unsupervised.html">unsupervised</a></span>
        </div>
        <div class="comments">
            <a href="http://neupy.com/2015/09/20/discrete_hopfield_network.html#disqus_thread" data-disqus-identifier="2015/09/20/discrete_hopfield_network">Leave a comment</a>
        </div></div><div class="separator post_separator"></div><div class="timestamp postmeta">
            <span>July 04, 2015</span>
        </div>
        <div class="section">
            <h1><a href="2015/07/04/boston_house_prices_dataset.html">Boston house-prices dataset<a class="headerlink" href="#boston-house-prices-dataset" title="Permalink to this headline">¶</a></a></h1>
<p>For this article we are going to predict Boston house prices via Conjugate gradient.</p>
<p>For the beginning we should get some data.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_boston</span><span class="p">()</span>
<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
<p>Let’s look closer into the data.</p>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.00632</td>
      <td>18</td>
      <td>2.31</td>
      <td>0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1</td>
    </tr>
    <tr>
      <td>0.02731</td>
      <td>0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2</td>
    </tr>
    <tr>
      <td>0.02729</td>
      <td>0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2</td>
    </tr>
    <tr>
      <td>0.03237</td>
      <td>0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3</td>
    </tr>
    <tr>
      <td>0.06905</td>
      <td>0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>296</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <td>242</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <td>242</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <td>222</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <td>222</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
  </tbody>
</table><p>Data contains 14 columns.
The last column <span class="docutils literal"><span class="pre">MEDV</span></span> is a median value of owner-occupied homes in $1000’s.
The goal is to predict this prices.
Other columns we can use for Neural Network training.
All columns description you can find below.</p>
<ul class="simple">
<li>CRIM     per capita crime rate by town</li>
<li>ZN       proportion of residential land zoned for lots over 25,000 sq.ft.</li>
<li>INDUS    proportion of non-retail business acres per town</li>
<li>CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</li>
<li>NOX      nitric oxides concentration (parts per 10 million)</li>
<li>RM       average number of rooms per dwelling</li>
<li>AGE      proportion of owner-occupied units built prior to 1940</li>
<li>DIS      weighted distances to five Boston employment centres</li>
<li>RAD      index of accessibility to radial highways</li>
<li>TAX      full-value property-tax rate per $10,000</li>
<li>PTRATIO  pupil-teacher ratio by town</li>
<li>B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</li>
<li>LSTAT    % lower status of the population</li>
</ul>
<p>From data set description we can find that there are 13 continuous attributes (including “class” attribute “MEDV”) and 1 binary-valued attribute.
There is no multiple categorical data, so we don’t need to change feature dimension.
But we already have one problem.
If you look closer, you will find that every column has its own data range.
This situation is a bad thing for Neural Network training, because input values ​​make different contributions to the calculation of the output values.
Bigger values will be more important for Network which can be perceived as invalid assumption based on data.
For example in the first row, in the table above, column <span class="docutils literal"><span class="pre">B</span></span> contains value <cite>396.90</cite> and column <span class="docutils literal"><span class="pre">CRIM</span></span> - <cite>0.00632</cite>.
To fix this issue we should transfrom all columns to get similar ranges.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>

<span class="n">data_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">target_scaler</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">MinMaxScaler</span><span class="p">()</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">data_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">target_scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
<p>After transformation data looks like this.</p>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>...</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.000000</td>
      <td>0.18</td>
      <td>0.067815</td>
      <td>0</td>
      <td>0.314815</td>
      <td>...</td>
    </tr>
    <tr>
      <td>0.000236</td>
      <td>0.00</td>
      <td>0.242302</td>
      <td>0</td>
      <td>0.172840</td>
      <td>...</td>
    </tr>
    <tr>
      <td>0.000236</td>
      <td>0.00</td>
      <td>0.242302</td>
      <td>0</td>
      <td>0.172840</td>
      <td>...</td>
    </tr>
    <tr>
      <td>0.000293</td>
      <td>0.00</td>
      <td>0.063050</td>
      <td>0</td>
      <td>0.150206</td>
      <td>...</td>
    </tr>
    <tr>
      <td>0.000705</td>
      <td>0.00</td>
      <td>0.063050</td>
      <td>0</td>
      <td>0.150206</td>
      <td>...</td>
    </tr>
  </tbody>
</table><p>All the data is now in the range between 0 and 1.</p>
<p>Then we should split our data set into train and validation.
We use 85% of data for train.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">environment</span>

<span class="n">environment</span><span class="o">.</span><span class="n">reproducible</span><span class="p">()</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.85</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Now we are ready to build Neural Network which will predict house prices.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">algorithms</span><span class="p">,</span> <span class="n">layers</span>

<span class="n">cgnet</span> <span class="o">=</span> <span class="n">algorithms</span><span class="o">.</span><span class="n">ConjugateGradient</span><span class="p">(</span>
    <span class="n">connection</span><span class="o">=</span><span class="p">[</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="mi">13</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">],</span>
    <span class="n">search_method</span><span class="o">=</span><span class="s1">'golden'</span><span class="p">,</span>
    <span class="n">show_epoch</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">addons</span><span class="o">=</span><span class="p">[</span><span class="n">algorithms</span><span class="o">.</span><span class="n">LinearSearch</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/cgnet-init.png"><img alt="Conjgate Gradient train" src="_images/cgnet-init.png" style="width: 80%;"/></a>
</div>
<p>We define network with one hidden layer.
Input size for this layer is 50.
This value is just a guess.
For better and more accurate result we should choose it with other methods, but for now we can use this value.
As the main algorithm we take Conjugate Gradient.
This implementation of backpropagation is a little bit different from main interpretation of Conjugate Gradient.
For GradientDescent implementation we can’t guarantee that we get the local minimum in n-th steps (where <cite>n</cite> is the dimension).
To optimize it we should use linear search.
It will fix and set up better steps for Conjugate Gradient.</p>
<p>Now we are going to train the network.
For training we set up 100 epochs.
Also we will add test data into training function to check validation error on every epoch.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="n">cgnet</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/cgnet-train.png"><img alt="Conjgate Gradient train" src="_images/cgnet-train.png" style="width: 80%;"/></a>
</div>
<p>To make sure that all training processes go in a right way we can check erros updates while the training is in process.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy</span> <span class="kn">import</span> <span class="n">plots</span>
<span class="n">plots</span><span class="o">.</span><span class="n">error_plot</span><span class="p">(</span><span class="n">cgnet</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/cgnet-error-plot.png"><img alt="Conjgate Gradient train" src="_images/cgnet-error-plot.png" style="width: 80%;"/></a>
</div>
<p>Error minimization procedure looks fine.
The problem is, that last error doesn’t show us the full picture of prediction accuracy.
Our output is always between zero and one and we count the results always into Mean Square Error.
To fix it, we are going to inverse our transformation for predicted and actual values and for accuracy measurment we will use Root Mean Square Logarithmic Error (RMSLE).</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">neupy.estimators</span> <span class="kn">import</span> <span class="n">rmsle</span>

<span class="n">y_predict</span> <span class="o">=</span> <span class="n">cgnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">rmsle</span><span class="p">(</span><span class="n">target_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_test</span><span class="p">),</span>
              <span class="n">target_scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_predict</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we can see that our error approximately equals to <cite>0.22</cite> which is pretty small.
In the table below you can find 10 randomly chosen errors.</p>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>Actual</th>
      <th>Predicted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>31.2</td>
      <td>27.5</td>
    </tr>
    <tr>
      <td>18.7</td>
      <td>18.5</td>
    </tr>
    <tr>
      <td>20.1</td>
      <td>18.5</td>
    </tr>
    <tr>
      <td>17.2</td>
      <td>9.5</td>
    </tr>
    <tr>
      <td>8.3</td>
      <td>9.5</td>
    </tr>
    <tr>
      <td>50.0</td>
      <td>41.0</td>
    </tr>
    <tr>
      <td>42.8</td>
      <td>32.0</td>
    </tr>
    <tr>
      <td>20.5</td>
      <td>18.5</td>
    </tr>
    <tr>
      <td>16.8</td>
      <td>23.0</td>
    </tr>
    <tr>
      <td>11.8</td>
      <td>9.5</td>
    </tr>
  </tbody>
</table><p>The results are good for the first network implementation.
There are a lot of things which we can do to improve network results, but we will discuss them in an another article.</p>

        </div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="tags/supervised.html">supervised</a>, <a href="tags/backpropagation.html">backpropagation</a>, <a href="tags/regression.html">regression</a></span>
        </div>
        <div class="comments">
            <a href="http://neupy.com/2015/07/04/boston_house_prices_dataset.html#disqus_thread" data-disqus-identifier="2015/07/04/boston_house_prices_dataset">Leave a comment</a>
        </div></div><div class="separator post_separator"></div><div class="timestamp postmeta">
            <span>July 04, 2015</span>
        </div>
        <div class="section">
            <h1><a href="2015/07/04/visualize_backpropagation_algorithms.html"><a class="toc-backref" href="#id3">Visualize Algorithms based on the Backpropagation</a><a class="headerlink" href="#visualize-algorithms-based-on-the-backpropagation" title="Permalink to this headline">¶</a></a></h1>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#visualize-algorithms-based-on-the-backpropagation" id="id3">Visualize Algorithms based on the Backpropagation</a><ul>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#checking-data" id="id4">Checking data</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#initialize-contour" id="id5">Initialize contour</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#id1" id="id6">Visualize algorithms based on the Backpropagation</a><ul>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#gradient-descent" id="id7">Gradient Descent</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#momentum" id="id8">Momentum</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#rprop" id="id9">RPROP</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#irprop" id="id10">iRPROP+</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#gradient-descent-and-golden-search" id="id11">Gradient Descent and Golden Search</a></li>
</ul>
</li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#bring-them-all-together" id="id12">Bring them all together</a></li>
<li><a class="reference internal" href="2015/07/04/visualize_backpropagation_algorithms.html#summary" id="id13">Summary</a></li>
</ul>
</li>
</ul>
</div>
<p>In this article we will be testing different algorithms based on the backpropagation method, visualizing them and trying to figure out some important features from a blog that we will get.</p>
<div class="section" id="checking-data">
<h2><a class="toc-backref" href="#id4">Checking data</a><a class="headerlink" href="#checking-data" title="Permalink to this headline">¶</a></h2>
<p>First of all we need to define simple dataset which contains 6 points with two features.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
<span class="p">])</span>
<span class="n">target_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
<span class="p">])</span>
</pre></div>
</div>
<p>So we can make a scatter plot and look closer at this dots.</p>
<div class="highlight-python"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">input_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">input_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">target_data</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/bp-vis-scatter.png"><img alt="Dataset scatter plot" src="_images/bp-vis-scatter.png" style="width: 80%;"/></a>
</div>
<p>From the figure above we can clearly see that all dots are linearly separable and we are able to solve this problem with simple perceptron. But the goal of this article is to make clear visualization of learning process for different algorithm based on the backpropagation method, so the problem has to be as simple as possible, because in other cases it will be complex to visualize.</p>
<p>So, since the problem is linear separable we can solve it without hidden layers in network. There are two features and two classes, so we can build network which will take 2 input values and will produce 1 output. We need just two weights, so we can visualize them in contour plot.</p>
</div>
<div class="section" id="initialize-contour">
<h2><a class="toc-backref" href="#id5">Initialize contour</a><a class="headerlink" href="#initialize-contour" title="Permalink to this headline">¶</a></h2>
<p>I won’t  add all code related to the plots building in the article. In case if you are interested you can check the main script <a class="reference external" href="https://github.com/itdxer/neupy/blob/master/examples/mlp/gd_algorithms_visualization.py">here</a>.</p>
<a class="reference internal image-reference" href="_images/raw-contour-plot.png"><img alt="Approximation function contour plot" class="align-center" src="_images/raw-contour-plot.png" style="width: 80%;"/></a>
<p>The plot above shows error rate that depends on the network’s weights. The best result corresponds to the smallest error value. The best weights combination for this problem should be near the bottom right corner in the white area.</p>
<p>Next, we are going to look at 5 algorithms based on the Backpropagation. They are:</p>
<ul class="simple">
<li>Gradient descent</li>
<li>Momentum</li>
<li>RPROP</li>
<li>iRPROP+</li>
<li>Gradient Descent + Golden Search</li>
</ul>
<p>Let’s define start point for our algorithms. I’ve chosen the <cite>(-4, -4)</cite> point, because at this point network gives bad results and it will be interesting to observe the learning progress from a bad initialization point. In the script you can set up any other starting point you like.</p>
<p>This function will train the network until the error will be smaller than <cite>0.125</cite>. Every network starts at place with coordinates <cite>(-4, -4)</cite> and finishes near the point with the error value lower than <cite>0.125</cite>.</p>
</div>
<div class="section" id="id1">
<h2><a class="toc-backref" href="#id6">Visualize algorithms based on the Backpropagation</a><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<div class="section" id="gradient-descent">
<h3><a class="toc-backref" href="#id7">Gradient Descent</a><a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h3>
<p>Let’s primarily check <a class="reference internal" href="apidocs/neupy.algorithms.gd.base.html#neupy.algorithms.gd.base.GradientDescent" title="neupy.algorithms.gd.base.GradientDescent"><span class="xref py py-class docutils literal"><span class="pre">Gradient</span> <span class="pre">Descent</span></span></a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/bp-steps.png"><img alt="Weight update steps for the Gradient Descent" src="_images/bp-steps.png" style="width: 80%;"/></a>
</div>
<p>Gradient Descent got to the value close to 0.125 using 797 steps and this black curve is just tiny steps of gradient descent algorithm. We can zoom it and look even closer.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/bp-steps-zoom.png"><img alt="Zoomed weight update steps for the Gradient Descent" src="_images/bp-steps-zoom.png" style="width: 80%;"/></a>
</div>
<p>Now we can see some information about gradient descent algorithm. All steps for gradient descent algorithm have approximately similar magnitude. Their direction doesn’t vary because contours in the zoomed picture are parallel to each other and in it we can see that there are still a lot of steps that are needed to be made to achieve the minimum. Also we can see that small vectors are perpendicular to the contour.</p>
<p>The problem is that the step size is a very sensitive parameter for the gradient descent. In typical problem we won’t be able to visualize the learning progress and we won’t have an ability to see that our updates over the epochs are inefficient. For this result I’ve used step size equal to <span class="docutils literal"><span class="pre">0.3</span></span>, but if we increased it to <span class="docutils literal"><span class="pre">10</span></span> we would reach our goal in <span class="docutils literal"><span class="pre">25</span></span> steps. I haven’t added any improvements to make a fair comparison to other algorithms in the summary chapter.</p>
</div>
<div class="section" id="momentum">
<h3><a class="toc-backref" href="#id8">Momentum</a><a class="headerlink" href="#momentum" title="Permalink to this headline">¶</a></h3>
<p>Now let’s look at another very popular algorithm - <a class="reference internal" href="apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/momentum-steps.png"><img alt="Momentum steps" src="_images/momentum-steps.png" style="width: 80%;"/></a>
</div>
<p><a class="reference internal" href="apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> got to the value close to 0.125 by 92 steps, which is more than 8 times less than for the gradient descent. The basic idea behind <a class="reference internal" href="apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> algorithm is that it accumulates gradients from the previous epochs. It means that if the gradient has the same direction after each epoch weight update vector magnitude will increase. But if the gradient stars changing its direction weight update vector magnitude will decrease. Check the figure again. Imagine that you’re standing at a skatepark. Than you throw a ball into a half-pipe in a way that makes it roll smoothly on the surface. While it rolls down the gravity force drags it down and it makes the ball roll faster and faster. Let’s get back to the <a class="reference internal" href="apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> algorithm and try to find these properties in the plot.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/momentum-steps-zoom.png"><img alt="Momentum steps zoom on increasing weight update size" src="_images/momentum-steps-zoom.png" style="width: 80%;"/></a>
</div>
<p>When we zoom the plot we can see that the direction for weight update vectors is almost the same and gradient’s direction doesn’t change after every epoch. In the picture above the vector which is the last on the right is bigger than the first one on the same plot on the left. Since it always moves forward it speeds up.</p>
<p>Let’s get back to the ball example. What happens when the ball reaches the pit of the half-pipe for the first time? Will it stop? Of course not. Ball gained enough speed for moving. So it will go up. But after that the ball will start to slow down and its amplitude will become smaller and smaller, because of the gravity force, that will continue to push it down to the pit and eventually it will stop to move. Let’s try to find the similar behavior in the same plot.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/momentum-steps-zoom-decrease.png"><img alt="Momentum steps zoom on decreasing weight update size" src="_images/momentum-steps-zoom-decrease.png" style="width: 80%;"/></a>
</div>
<p>From the figure above it’s clear that weight update magnitude became smaller. Like a ball that slows down and changes its direction towards the minimum.</p>
<p>And finally to make it even more intuitive you can check weight update trajectory in 3D plot. It looks much more like the ball and half-pipe in skatepark analogy.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/momentum-3d-trajectory.png"><img alt="Momentum 3D trajectory" src="_images/momentum-3d-trajectory.png" style="width: 80%;"/></a>
</div>
</div>
<div class="section" id="rprop">
<h3><a class="toc-backref" href="#id9">RPROP</a><a class="headerlink" href="#rprop" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> makes fewer steps to reach the specified minimum point, but we still can do better. Next algorithm that we are going to check is <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/rprop-steps.png"><img alt="RPROP steps" src="_images/rprop-steps.png" style="width: 80%;"/></a>
</div>
<p>This improvement looks impressive. Now we are able to see steps without zooming. We got almost the same value as before using just 20 steps, which is approximately 5 times less than <a class="reference internal" href="apidocs/neupy.algorithms.gd.momentum.html#neupy.algorithms.gd.momentum.Momentum" title="neupy.algorithms.gd.momentum.Momentum"><span class="xref py py-class docutils literal"><span class="pre">Momentum</span></span></a> and approximately 40 times less than <a class="reference internal" href="apidocs/neupy.algorithms.gd.base.html#neupy.algorithms.gd.base.GradientDescent" title="neupy.algorithms.gd.base.GradientDescent"><span class="xref py py-class docutils literal"><span class="pre">Gradient</span> <span class="pre">Descent</span></span></a>.</p>
<p>Now we are going to figure out what are the main features of <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a>. We can notice just by looking at the plot above <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> has a unique step for each weight. There are just two steps for each weight in the input layer for this network. <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> will increase the step size if gradient don’t change the sign compare to previous epoch, and it will decrease otherwise.</p>
<p>Let’s check a few first weight updates.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/rprop-first-11-steps.png"><img alt="RPROP first 11 steps" src="_images/rprop-first-11-steps.png" style="width: 80%;"/></a>
</div>
<p>From the figure above you can see that first 11 updates have the same direction, so both steps increase their value after each iteration. For the first epoch steps are equal to the same value which we set up at network initialization step. In further iterations they increased by the same constant factor, so after six iteration they got bigger, but they are still equal because they move in one direction all the time.</p>
<p>Now let’s check the next epochs from the figure below. At the 12th epoch gradient changed the direction, but steps are still the same in value. But we can clearly see that gradient changed the sign for the second weight. <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> updated the step after weight had updated, so the step for the second weight should be smaller for the 13th epoch.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/rprop-11th-to-14th-epochs.png"><img alt="RPROP from 11th to 14th steps" src="_images/rprop-11th-to-14th-epochs.png" style="width: 80%;"/></a>
</div>
<p>Now let’s look at the 13th epoch. It shows us how gradient sign difference at the 12th epoch updated steps. Now the steps are not equal. From the picture above we can see that update on the second weight (y axis) is smaller than on the first weight (x axis).</p>
<p>At the 16th epoch gradient on y axis changed the sign again. Network decreased by constant factor and updated for the second weight at the 17th epoch would be smaller than at the 16th.</p>
<p>To train your intuition you can check the other epochs updates and try to figure out how steps depend on the direction.</p>
</div>
<div class="section" id="irprop">
<h3><a class="toc-backref" href="#id10">iRPROP+</a><a class="headerlink" href="#irprop" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.IRPROPPlus" title="neupy.algorithms.gd.rprop.IRPROPPlus"><span class="xref py py-class docutils literal"><span class="pre">iRPROP+</span></span></a> is almost the same algorithm as <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> except a small alteration.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/irprop-plus-steps.png"><img alt="iRPROP+ steps" src="_images/irprop-plus-steps.png" style="width: 80%;"/></a>
</div>
<p>As in <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> algorithm <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.IRPROPPlus" title="neupy.algorithms.gd.rprop.IRPROPPlus"><span class="xref py py-class docutils literal"><span class="pre">iRPROP+</span></span></a> make exactly the same first 11 steps.</p>
<p>Now let’s look at the 12th step in the figure below.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/irprop-plus-second-part.png"><img alt="iRPROP+ second part" src="_images/irprop-plus-second-part.png" style="width: 80%;"/></a>
</div>
<p>Second weight (on the y axis) didn’t change the value. At the same epoch <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> changed the gradient comparing to the previous epoch and just decreased step value after weight update whereas, <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.IRPROPPlus" title="neupy.algorithms.gd.rprop.IRPROPPlus"><span class="xref py py-class docutils literal"><span class="pre">iRPROP+</span></span></a> disabled weight update for current epoch (set it up to <cite>0</cite>). And of course it also decreased the step for the second weight. Also you can find that vector for the 12th epoch that looks smaller than for the <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> algorithm, because we ignored the second weight update. If we check the x axis update size we will find that it has the same value as in <a class="reference internal" href="apidocs/neupy.algorithms.gd.rprop.html#neupy.algorithms.gd.rprop.RPROP" title="neupy.algorithms.gd.rprop.RPROP"><span class="xref py py-class docutils literal"><span class="pre">RPROP</span></span></a> algorithm.</p>
<p>At 13th epoch network again included second weight into the update process, because compared to the previous epoch gradient didn’t change its sign.</p>
<p>The nice thing about this algorithm is that it tries to move in a new direction instead of going back and force and trying to redo updates from the previous epochs.</p>
</div>
<div class="section" id="gradient-descent-and-golden-search">
<h3><a class="toc-backref" href="#id11">Gradient Descent and Golden Search</a><a class="headerlink" href="#gradient-descent-and-golden-search" title="Permalink to this headline">¶</a></h3>
<p>The last algorithm that I want to show is a <a class="reference internal" href="apidocs/neupy.algorithms.step_update.linear_search.html#neupy.algorithms.step_update.linear_search.LinearSearch" title="neupy.algorithms.step_update.linear_search.LinearSearch"><span class="xref py py-class docutils literal"><span class="pre">Golden</span> <span class="pre">Search</span></span></a>. This algorithm is not able to train a network by itself, but it can help other algorithms to do it better. I will use Gradient Descent to show the huge improvement that gives <a class="reference internal" href="apidocs/neupy.algorithms.step_update.linear_search.html#neupy.algorithms.step_update.linear_search.LinearSearch" title="neupy.algorithms.step_update.linear_search.LinearSearch"><span class="xref py py-class docutils literal"><span class="pre">Golden</span> <span class="pre">Search</span></span></a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/grad-descent-and-gold-search-steps.png"><img alt="Gradient Descent with Golden Search steps" src="_images/grad-descent-and-gold-search-steps.png" style="width: 80%;"/></a>
</div>
<p>It took just two steps to reach the goal. Let’s check the first step. <a class="reference internal" href="apidocs/neupy.algorithms.step_update.linear_search.html#neupy.algorithms.step_update.linear_search.LinearSearch" title="neupy.algorithms.step_update.linear_search.LinearSearch"><span class="xref py py-class docutils literal"><span class="pre">Golden</span> <span class="pre">Search</span></span></a> helps to find the best step size that can be in a specified direction. So basically, it just tries multiple combinations until it finds the best one. As you can see from the plot the first step size is almost perfect for the specified direction. If you went farther you would increase the error.</p>
<p>The main disadvantage of <a class="reference internal" href="apidocs/neupy.algorithms.step_update.linear_search.html#neupy.algorithms.step_update.linear_search.LinearSearch" title="neupy.algorithms.step_update.linear_search.LinearSearch"><span class="xref py py-class docutils literal"><span class="pre">Golden</span> <span class="pre">Search</span></span></a> is a time complexity. It will take a while to find a good step in specified direction. So for the more complicated networks it can take a lot of time to find a perfect step size.</p>
</div>
</div>
<div class="section" id="bring-them-all-together">
<h2><a class="toc-backref" href="#id12">Bring them all together</a><a class="headerlink" href="#bring-them-all-together" title="Permalink to this headline">¶</a></h2>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/all-algorithms-steps.png"><img alt="All algorithms steps" src="_images/all-algorithms-steps.png" style="width: 80%;"/></a>
</div>
</div>
<div class="section" id="summary">
<h2><a class="toc-backref" href="#id13">Summary</a><a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<table border="1" class="docutils" id="id2">
<caption><span class="caption-text">Summary table</span><a class="headerlink" href="#id2" title="Permalink to this table">¶</a></caption>
<colgroup>
<col width="50%"/>
<col width="50%"/>
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Algorithm</th>
<th class="head">Number of epochs</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Gradient Descent</td>
<td>797</td>
</tr>
<tr class="row-odd"><td>Momentum</td>
<td>92</td>
</tr>
<tr class="row-even"><td>RPROP</td>
<td>20</td>
</tr>
<tr class="row-odd"><td>iRPROP+</td>
<td>17</td>
</tr>
<tr class="row-even"><td>Gradient Descent + Golden Search</td>
<td>2</td>
</tr>
</tbody>
</table>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/compare-number-of-epochs.png"><img alt="Compare number of epochs" src="_images/compare-number-of-epochs.png" style="width: 80%;"/></a>
</div>
<p>There is no perfect algorithm for neural network that can solve all problems. All of them have their own pros and cons. Some of the algorithms can be memory or computationally overwhelming and you have to choose an algorithm depending on the task you want to solve.</p>
<p>All code is available at <a class="reference external" href="https://github.com/itdxer/neupy/blob/master/examples/mlp/gd_algorithms_visualization.py">GitHub</a>. You can play around with the script and set up different learning algorithms and hyperparameters. More algorithms you can find at NeuPy’s <a class="reference internal" href="docs/cheatsheet.html#cheat-sheet"><span>Cheat sheet</span></a>.</p>
</div>

        </div>
        <div class="postmeta">
        <div class="author">
            <span>Posted by Yurii Shevchuk</span>
        </div>
        
        <div class="tags">
            <span>
                Tags:
                <a href="tags/supervised.html">supervised</a>, <a href="tags/backpropagation.html">backpropagation</a>, <a href="tags/visualization.html">visualization</a></span>
        </div>
        <div class="comments">
            <a href="http://neupy.com/2015/07/04/visualize_backpropagation_algorithms.html#disqus_thread" data-disqus-identifier="2015/07/04/visualize_backpropagation_algorithms">Leave a comment</a>
        </div></div><div class="archive_link">
        <a href="archive.html"> &mdash; Blog Archive &mdash; </a>
    </div></article><aside class="sidebar"><section><div class="widget">
    <h1>Recent Posts</h1>
    <ul><li>
            <a href="2015/09/21/password_recovery.html">Password recovery</a>
        </li><li>
            <a href="2015/09/20/discrete_hopfield_network.html">Discrete Hopfiel Network</a>
        </li><li>
            <a href="2015/07/04/boston_house_prices_dataset.html">Boston house-prices dataset</a>
        </li><li>
            <a href="2015/07/04/visualize_backpropagation_algorithms.html">Visualize Algorithms based on the Backpropagation</a>
        </li></ul>
</div>
</section><section><div class="widget">
    <h1>Cheat sheet</h1>
    <ul>
        <li><a href="docs/cheatsheet.html#algorithms">Algorithms</a></li>
        <li><a href="docs/cheatsheet.html#layers">Layers</a></li>
        <li><a href="docs/cheatsheet.html#parameter-initialization-methods">Parameter Initialization Methods</a></li>
        <li><a href="docs/cheatsheet.html#error-functions">Error functions</a></li>
    </ul>
</div></section><section><div class="widget">
    <h1>Installation</h1>
    <div class="highligh-bash">
        <div class="highlight">
            <pre>pip install neupy</pre>
        </div>
    </div>
    <p>Read more in <a href="pages/documentation.html">Documentation</a>.</p>
    <p>Old documentations you can find <a href="pages/versions.html">here</a>.</p>
</div></section><section><div class="widget" id="searchbox" role="search">
    <h1><a href="#searchbox">Search</a></h1>
    <form action="search.html" method="get">
        <input type="text" name="q" />
        <button type="submit"><span class="fa fa-search"></span></button>
    </form>
</div></section></aside></div> <!-- #main --></div> <!-- #main-container -->

        <div class="footer-container" role="contentinfo"><footer class="wrapper">&copy; Copyright 2015 - 2016, Yurii Shevchuk. Powered by <a href="http://www.tinkerer.me/">Tinkerer</a> and <a href="http://sphinx.pocoo.org/">Sphinx</a>.</footer>
    <a id="fork_me" href="http://github.com/itdxer/neupy">
        <img alt="Fork me" src="_static/img/github-fork-green.png" />
    </a></div> <!-- footer-container -->

      </div> <!--! end of #container --><script type="text/javascript">    var disqus_shortname = "neupy";    disqus_count();</script><!--[if lt IE 7 ]>
          <script src="//ajax.googleapis.com/ajax/libs/chrome-frame/1.0.3/CFInstall.min.js"></script>
          <script>window.attachEvent('onload',function(){CFInstall.check({mode:'overlay'})})</script>
        <![endif]-->
    </body>
</html>